pute# Quick Start Guide - Proiect Reinforcement Learning

Ghid rapid pentru a Ã®ncepe sÄƒ lucrezi cu proiectul de RL.

---

## Instalare RapidÄƒ

### Windows

```bash
# 1. NavigheazÄƒ Ã®n directorul proiectului
cd C:\Users\Horia\PyCharmMiscProject\proiect_irl

# 2. ActiveazÄƒ virtual environment
.venv\Scripts\activate

# 3. InstaleazÄƒ dependenÈ›ele (dacÄƒ nu ai fÄƒcut-o deja)
pip install -r requirements.txt
```

### Linux/Mac

```bash
# 1. NavigheazÄƒ Ã®n directorul proiectului
cd /path/to/proiect_irl

# 2. ActiveazÄƒ virtual environment
source .venv/bin/activate

# 3. InstaleazÄƒ dependenÈ›ele (dacÄƒ nu ai fÄƒcut-o deja)
pip install -r requirements.txt
```

**Verificare instalare:**
```python
python -c "import gymnasium, torch, stable_baselines3; print('âœ“ All packages installed!')"
```

---

## Rulare Experimente - 4 OpÈ›iuni Principale

### OpÈ›iunea 0: Test Rapid - ToÈ›i Algoritmii pe EasyFrozenLake (2-3 minute) âš¡

VerificÄƒ rapid cÄƒ toÈ›i cei 5 algoritmi funcÈ›ioneazÄƒ pe mediul simplu:

```bash
cd experiments
python test_easy_env.py
```

**Ce face:**
- AntreneazÄƒ Q-Learning (300 episoade)
- AntreneazÄƒ DQN (300 episoade)
- AntreneazÄƒ DQN+PER (300 episoade)
- AntreneazÄƒ PPO (15,000 timesteps)
- AntreneazÄƒ PPO+RND (15,000 timesteps)
- EvalueazÄƒ fiecare agent (50 episoade)
- AfiÈ™eazÄƒ tabel comparativ cu rezultate

**Output aÈ™teptat:**
```
COMPARATIE FINALA - TOÈšI ALGORITMII
============================================================
Algorithm       Success Rate    Mean Steps   Mean Reward
------------------------------------------------------------
DQN+PER         100.0%     ğŸ†   6.5          1.19
PPO             100.0%     ğŸ†   6.6          1.19
PPO+RND         100.0%     ğŸ†   6.7          1.18
Q-Learning      100.0%     ğŸ†   6.8          1.18
DQN             35.0%           30.2         0.12

âœ“ 4/5 algoritmi au atins â‰¥80% success rate
ğŸ† CÃ¢È™tigÄƒtor: DQN+PER cu 100.0% success rate
```

---

### OpÈ›iunea 1: Benchmark Complet (5-10 minute) ğŸ†

AntreneazÄƒ È™i evalueazÄƒ toÈ›i cei 5 algoritmi:

```bash
cd experiments
python benchmark_all_agents.py
```

**Ce face:**
- AntreneazÄƒ Q-Learning (500 episoade)
- AntreneazÄƒ DQN (500 episoade)
- AntreneazÄƒ DQN+PER (500 episoade) â­
- AntreneazÄƒ PPO (25,000 timesteps)
- AntreneazÄƒ PPO+RND (25,000 timesteps)
- EvalueazÄƒ fiecare agent (100 episoade)
- SalveazÄƒ rezultate JSON Ã®n `results/benchmark_easy_TIMESTAMP.json`

**Rezultate aÈ™teptate:**

| Algorithm | Success Rate | Mean Steps | Efficiency Score |
|-----------|--------------|------------|------------------|
| Q-Learning | 100% | 6.54 | 15.29 |
| DQN | 32% | 32.76 | 0.98 |
| **DQN+PER** ğŸ† | **100%** | **6.37** | **15.70** |
| PPO | 100% | 6.38 | 15.67 |
| PPO+RND | 100% | 6.40 | 15.62 |

**Pentru test rapid (1-2 min)**, modificÄƒ Ã®n `benchmark_all_agents.py`:
```python
N_EPISODES = 100        # Ãn loc de 500
PPO_TIMESTEPS = 5000    # Ãn loc de 25000
N_EVAL = 20             # Ãn loc de 100
```

---

### OpÈ›iunea 2: Vizualizare Rezultate (10 secunde) ğŸ“Š

GenereazÄƒ grafice din ultimul benchmark:

```bash
cd experiments
python visualize_benchmark.py
```

**Grafice generate (Ã®n `results/`):**
1. `benchmark_comparison.png` - ComparaÈ›ie 3 metrici (Success Rate, Reward, Steps)
2. `learning_curves.png` - EvoluÈ›ia rewardurilor Ã®n timpul training-ului
3. `efficiency_scatter.png` - Scatter plot Success Rate vs Mean Steps
4. `winner_ranking.png` - Ranking cu DQN+PER ca cÃ¢È™tigÄƒtor

---

### OpÈ›iunea 3: Multi-Seed Analysis (15-20 minute) ğŸ”¬

TesteazÄƒ reproducibilitatea pe 5 seed-uri diferite:

```bash
cd experiments
python benchmark_multi_seed.py
```

**Ce face:**
- RuleazÄƒ toÈ›i cei 5 algoritmi pe seeds: [42, 123, 456, 789, 1024]
- CalculeazÄƒ mean Â± std pentru fiecare metric
- IdentificÄƒ seed-uri problematice (789, 1024 sunt dificile)
- SalveazÄƒ `results/benchmark_multi_seed_TIMESTAMP.json`

**Apoi vizualizeazÄƒ:**
```bash
python visualize_multi_seed.py
```

**Grafice generate:**
1. `multi_seed_comparison.png` - Mean Â± std bars pentru 3 metrici
2. `multi_seed_stability.png` - ComparaÈ›ie deviaÈ›ii standard
3. `multi_seed_distribution.png` - DistribuÈ›ie rezultate per seed

**Rezultate aÈ™teptate (mean Â± std):**

| Algorithm | Success Rate | Stabilitate |
|-----------|--------------|-------------|
| **PPO** ğŸ† | **98.60% Â± 2.33%** | Foarte stabil |
| PPO+RND | 98.60% Â± 2.33% | Foarte stabil |
| DQN+PER | 80.20% Â± 39.60% | Instabil |
| Q-Learning | 66.60% Â± 42.22% | Instabil |
| DQN | 41.20% Â± 44.93% | Foarte instabil |

---

## Training Custom - Exemple Practice

### Exemplu 1: Q-Learning pe Map Custom

```python
from environments.easy_frozenlake import EasyFrozenLakeEnv
from agents.q_learning import QLearningAgent

# CreeazÄƒ mediu custom (mai dificil)
env = EasyFrozenLakeEnv(
    map_size=4,
    slippery=0.10,        # CreÈ™te alunecare (default 0.05)
    hole_ratio=0.15,      # Mai multe gÄƒuri (default 0.10)
    shaped_rewards=True,
    seed=42
)

# CreeazÄƒ agent
agent = QLearningAgent(
    n_states=env.observation_space.n,
    n_actions=env.action_space.n,
    learning_rate=0.1,
    discount_factor=0.99
)

# Training
from tqdm import tqdm
for episode in tqdm(range(500), desc="Training Q-Learning"):
    stats = agent.train_episode(env)

# Evaluare
eval_stats = agent.evaluate(env, n_episodes=100)
print(f"\nSuccess Rate: {eval_stats['success_rate']:.2%}")
print(f"Mean Steps: {eval_stats['mean_steps']:.2f}")

# Salvare agent antrenat
agent.save("models/q_learning_custom.pkl")
```

---

### Exemplu 2: DQN+PER (Cel Mai Eficient) â­

```python
from agents.dqn_per import DQN_PERAgent
from tqdm import tqdm

# CreeazÄƒ agent DQN+PER (best performer)
agent = DQN_PERAgent(
    n_states=16,
    n_actions=4,
    learning_rate=0.001,
    per_alpha=0.6,          # Prioritizare experienÈ›e
    per_beta_start=0.4,     # Importance sampling
    buffer_capacity=10000,
    seed=42
)

# Training cu progress bar
for episode in tqdm(range(500), desc="Training DQN+PER"):
    stats = agent.train_episode(env)

    # Logging periodic
    if (episode + 1) % 100 == 0:
        eval_stats = agent.evaluate(env, n_episodes=10)
        print(f"\nEpisode {episode+1}: Success Rate = {eval_stats['success_rate']:.1%}")

# Evaluare finalÄƒ
final_stats = agent.evaluate(env, n_episodes=100)
print(f"\n=== Final Results ===")
print(f"Success Rate: {final_stats['success_rate']:.2%}")  # Expected: 100%
print(f"Mean Steps: {final_stats['mean_steps']:.2f}")     # Expected: ~6.37

# Salvare
agent.save("models/dqn_per_best.pth")
```

---

### Exemplu 3: PPO (Cel Mai Stabil) ğŸ¯

```python
from agents.ppo import PPOAgent

# CreeazÄƒ agent PPO (most stable)
agent = PPOAgent(
    env=env,
    learning_rate=3e-4,
    n_steps=512,
    batch_size=64,
    verbose=1  # Progress logging
)

# Training (timesteps nu episoade)
agent.train(total_timesteps=25000)

# Evaluare
eval_stats = agent.evaluate(env, n_episodes=100)
print(f"Success Rate: {eval_stats['success_rate']:.2%}")  # Expected: 100%
print(f"Mean Steps: {eval_stats['mean_steps']:.2f}")     # Expected: ~6.38

# Salvare
agent.save("models/ppo_stable.zip")
```

---

## ÃncÄƒrcare È™i Refolosire AgenÈ›i AntrenaÈ›i

### Q-Learning

```python
from agents.q_learning import QLearningAgent

# ÃncÄƒrcare
agent = QLearningAgent.load("models/q_learning_custom.pkl")

# Evaluare pe mediu nou
new_env = EasyFrozenLakeEnv(seed=123)
eval_stats = agent.evaluate(new_env, n_episodes=100)
print(f"Success Rate on new seed: {eval_stats['success_rate']:.2%}")
```

### DQN+PER

```python
from agents.dqn_per import DQN_PERAgent

# ÃncÄƒrcare
agent = DQN_PERAgent.load("models/dqn_per_best.pth")

# Evaluare
eval_stats = agent.evaluate(env, n_episodes=100)
```

### PPO

```python
from agents.ppo import PPOAgent

# ÃncÄƒrcare (necesitÄƒ env pentru compatibilitate Stable-Baselines3)
agent = PPOAgent.load("models/ppo_stable.zip", env=env)

# Evaluare
eval_stats = agent.evaluate(env, n_episodes=100)
```

---

## Modificare Parametri Mediu

### EasyFrozenLake - Niveluri de Dificultate

**Easy (default):**
```python
env = EasyFrozenLakeEnv(
    map_size=4,
    slippery=0.05,       # 5% alunecare
    hole_ratio=0.10,     # 10% gÄƒuri
    shaped_rewards=True
)
```

**Medium:**
```python
env = EasyFrozenLakeEnv(
    map_size=4,
    slippery=0.10,       # 10% alunecare
    hole_ratio=0.15,     # 15% gÄƒuri
    shaped_rewards=True
)
```

**Hard:**
```python
env = EasyFrozenLakeEnv(
    map_size=4,
    slippery=0.15,       # 15% alunecare
    hole_ratio=0.20,     # 20% gÄƒuri
    shaped_rewards=True,
    shaping_scale=0.03   # Reduce reward shaping
)
```

### DynamicFrozenLake (Challenge) - 8Ã—8

```python
from environments.dynamic_frozenlake import DynamicFrozenLakeEnv

# Mediu dificil cu dificultate crescÃ¢ndÄƒ
dynamic_env = DynamicFrozenLakeEnv(
    map_size=8,
    slippery_start=0.08,
    slippery_end=0.25,
    hole_ratio=0.18,
    ice_melting=True,
    shaped_rewards=True
)

# Recomandat: PPO cu training extins
agent = PPOAgent(dynamic_env)
agent.train(total_timesteps=100000)  # 100k timesteps
```

---

## Structura Output-urilor

DupÄƒ rularea experimentelor:

```
results/
â”œâ”€â”€ benchmark_easy_TIMESTAMP.json        # Date benchmark complet
â”‚   â”œâ”€â”€ Q-Learning: {training_rewards: [...], eval_stats: {...}}
â”‚   â”œâ”€â”€ DQN: {...}
â”‚   â”œâ”€â”€ DQN+PER: {...}
â”‚   â”œâ”€â”€ PPO: {...}
â”‚   â””â”€â”€ PPO+RND: {...}
â”‚
â”œâ”€â”€ benchmark_multi_seed_TIMESTAMP.json  # Date multi-seed analysis
â”‚
â””â”€â”€ Grafice PNG:
    â”œâ”€â”€ benchmark_comparison.png         # 3 metrici comparative
    â”œâ”€â”€ learning_curves.png              # Training progress
    â”œâ”€â”€ efficiency_scatter.png           # Scatter plot
    â”œâ”€â”€ winner_ranking.png               # Ranking cÃ¢È™tigÄƒtor
    â”œâ”€â”€ multi_seed_comparison.png        # Mean Â± std bars
    â”œâ”€â”€ multi_seed_stability.png         # Std comparison
    â””â”€â”€ multi_seed_distribution.png      # Per-seed distribution
```

---

## Probleme Comune & SoluÈ›ii

### 1. ModuleNotFoundError

**ProblemÄƒ:** `ModuleNotFoundError: No module named 'environments'`

**SoluÈ›ie:**
```bash
# AsigurÄƒ-te cÄƒ rulezi din directorul experiments/
cd experiments
python benchmark_all_agents.py
```

### 2. CUDA Out of Memory (DQN)

**ProblemÄƒ:** Eroare memorie GPU pentru DQN

**SoluÈ›ie:**
```python
# ForÈ›eazÄƒ CPU Ã®n agents/dqn.py sau agents/dqn_per.py
self.device = torch.device("cpu")
```

### 3. Virtual Environment Nu Se ActiveazÄƒ

**Windows:**
```bash
# DacÄƒ .venv\Scripts\activate nu funcÈ›ioneazÄƒ
python -m venv .venv
.venv\Scripts\activate.bat
```

**Linux/Mac:**
```bash
# DacÄƒ source .venv/bin/activate nu funcÈ›ioneazÄƒ
python3 -m venv .venv
source .venv/bin/activate
```

### 4. DependenÈ›e LipsÄƒ

**ProblemÄƒ:** Import errors pentru pachete

**SoluÈ›ie:**
```bash
# Reinstalare dependenÈ›e
pip install --upgrade pip
pip install -r requirements.txt
```

---

## Workflow Recomandat

### Pentru Testing Rapid

```python
# Test rapid un singur algoritm (1 min)
from environments.easy_frozenlake import EasyFrozenLakeEnv
from agents.dqn_per import DQN_PERAgent

env = EasyFrozenLakeEnv()
agent = DQN_PERAgent(16, 4)

# Training rapid
for episode in range(100):
    agent.train_episode(env)

# Evaluare
stats = agent.evaluate(env, n_episodes=20)
print(f"Success Rate: {stats['success_rate']:.1%}")
```

### Pentru Evaluare CompletÄƒ

```bash
# 1. Benchmark complet (toate algoritmii)
cd experiments
python benchmark_all_agents.py  # ~10 min

# 2. Vizualizare rezultate
python visualize_benchmark.py   # ~10 sec

# 3. AnalizÄƒ multi-seed (opÈ›ional)
python benchmark_multi_seed.py  # ~20 min
python visualize_multi_seed.py  # ~10 sec
```

### Pentru ComparaÈ›ii Custom

```python
# ComparÄƒ doar 2 algoritmi specifici
from environments.easy_frozenlake import EasyFrozenLakeEnv
from agents.q_learning import QLearningAgent
from agents.dqn_per import DQN_PERAgent
from tqdm import tqdm

env = EasyFrozenLakeEnv(seed=42)

# Train Q-Learning
ql_agent = QLearningAgent(16, 4)
for episode in tqdm(range(500), desc="Q-Learning"):
    ql_agent.train_episode(env)
ql_stats = ql_agent.evaluate(env, n_episodes=100)

# Train DQN+PER
dqn_agent = DQN_PERAgent(16, 4)
for episode in tqdm(range(500), desc="DQN+PER"):
    dqn_agent.train_episode(env)
dqn_stats = dqn_agent.evaluate(env, n_episodes=100)

# Compare
print(f"\nQ-Learning: {ql_stats['success_rate']:.1%} success")
print(f"DQN+PER: {dqn_stats['success_rate']:.1%} success")
```

---

## Tips & Tricks

### Monitorizare Training Ã®n Timp Real

```python
# Pentru Q-Learning/DQN/DQN+PER
from tqdm import tqdm

for episode in tqdm(range(500), desc="Training"):
    stats = agent.train_episode(env)

    if episode % 50 == 0:
        # Evaluare intermediarÄƒ
        eval_stats = agent.evaluate(env, n_episodes=10)
        print(f"\nEpisode {episode}: Success Rate = {eval_stats['success_rate']:.1%}")
```

### Salvare PeriodicÄƒ Checkpoints

```python
# Training cu checkpoints la fiecare 100 episoade
for episode in range(500):
    agent.train_episode(env)

    # Salvare checkpoint
    if (episode + 1) % 100 == 0:
        agent.save(f"models/checkpoint_ep{episode+1}.pkl")
        print(f"âœ“ Checkpoint saved at episode {episode+1}")
```

### Testare RapidÄƒ cu Parametri ReduÈ™i

Pentru debugging rapid, modificÄƒ parametrii Ã®n scripturi:

```python
# Ãn benchmark_all_agents.py
N_EPISODES = 100        # Ãn loc de 500 (5Ã— mai rapid)
PPO_TIMESTEPS = 5000    # Ãn loc de 25000 (5Ã— mai rapid)
N_EVAL = 20             # Ãn loc de 100 (5Ã— mai rapid)
```

---

## Structura Proiectului

```
proiect_irl/
â”œâ”€â”€ agents/                          # ImplementÄƒri algoritmi RL
â”‚   â”œâ”€â”€ q_learning.py               # Q-Learning tabular
â”‚   â”œâ”€â”€ dqn.py                      # Deep Q-Network
â”‚   â”œâ”€â”€ dqn_per.py                  # DQN + Prioritized Experience Replay
â”‚   â”œâ”€â”€ ppo.py                      # Proximal Policy Optimization
â”‚   â””â”€â”€ ppo_rnd.py                  # PPO + Random Network Distillation
â”‚
â”œâ”€â”€ environments/                    # Medii custom
â”‚   â”œâ”€â”€ easy_frozenlake.py          # FrozenLake 4Ã—4 simplificat
â”‚   â””â”€â”€ dynamic_frozenlake.py       # FrozenLake 8Ã—8 dinamic
â”‚
â”œâ”€â”€ experiments/                     # Scripturi experimentale
â”‚   â”œâ”€â”€ benchmark_all_agents.py     # Benchmark complet (MAIN)
â”‚   â”œâ”€â”€ benchmark_multi_seed.py     # Multi-seed analysis
â”‚   â”œâ”€â”€ visualize_benchmark.py      # Generare grafice benchmark
â”‚   â””â”€â”€ visualize_multi_seed.py     # Generare grafice multi-seed
â”‚
â”œâ”€â”€ results/                         # Output experimente
â”‚   â”œâ”€â”€ *.json                      # Rezultate numerice
â”‚   â””â”€â”€ *.png                       # Grafice generate
â”‚
â”œâ”€â”€ README.md                        # DocumentaÈ›ie completÄƒ
â”œâ”€â”€ QUICKSTART.md                    # Acest fiÈ™ier
â””â”€â”€ requirements.txt                 # DependenÈ›e Python
```

---

## Scripturi Disponibile - Rezumat

| Script | LocaÈ›ie | Descriere | Timp | Output |
|--------|---------|-----------|------|--------|
| `test_easy_env.py` | `experiments/` | Test rapid 5 algoritmi | ~3 min | Tabel comparativ |
| `benchmark_all_agents.py` | `experiments/` | Benchmark complet | ~10 min | JSON + logs |
| `visualize_benchmark.py` | `experiments/` | Generare 4 grafice | < 10s | 4 PNG files |
| `benchmark_multi_seed.py` | `experiments/` | Multi-seed (5 seeds) | ~20 min | JSON + statistici |
| `visualize_multi_seed.py` | `experiments/` | Grafice multi-seed | < 10s | 3 PNG files |
| `run_experiments.py` | `experiments/` | Script general custom | Variabil | Customizabil |

---

## Next Steps

DupÄƒ ce ai rulat experimentele cu succes:

1. **AnalizeazÄƒ graficele** din `results/` pentru insights
2. **ComparÄƒ algoritmii** - care e cel mai bun pentru task-ul tÄƒu?
3. **ExperimenteazÄƒ cu hiperparametri** diferiÈ›i
4. **ÃncearcÄƒ seed-uri noi** pentru testare robusteÈ›e
5. **ModificÄƒ mediul** pentru challenge-uri noi (map size, slippery, holes)
6. **ImplementeazÄƒ algoritmi noi** bazat pe arhitectura existentÄƒ

---

## Resurse Utile

### DocumentaÈ›ie Proiect
- **README.md** - DocumentaÈ›ie completÄƒ cu toate detaliile tehnice

### DocumentaÈ›ii Externe
- [Gymnasium Docs](https://gymnasium.farama.org/) - Framework environments
- [Stable-Baselines3 Docs](https://stable-baselines3.readthedocs.io/) - PPO implementation
- [PyTorch RL Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) - DQN tutorial
- [OpenAI Spinning Up](https://spinningup.openai.com/) - Deep RL educational resource

---

**Gata sÄƒ Ã®ncepi! Succes cu experimentele! ğŸš€**
