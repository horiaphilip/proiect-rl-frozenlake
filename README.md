# Proiect Reinforcement Learning - Dynamic FrozenLake

Proiect de Reinforcement Learning care implementeazÄƒ È™i comparÄƒ trei algoritmi diferiÈ›i (Q-Learning, DQN, PPO) pe un mediu personalizat FrozenLake dinamic.

## ğŸ“‹ Cuprins

- [Descriere](#descriere)
- [Structura Proiectului](#structura-proiectului)
- [Instalare](#instalare)
- [Utilizare](#utilizare)
- [Arhitectura Mediului](#arhitectura-mediului)
- [Algoritmi ImplementaÈ›i](#algoritmi-implementaÈ›i)
- [Rezultate](#rezultate)
- [Probleme ÃntÃ¢mpinate](#probleme-Ã®ntÃ¢mpinate)

## ğŸ¯ Descriere

Acest proiect exploreazÄƒ performanÈ›a È™i comportamentul a trei algoritmi de Reinforcement Learning Ã®ntr-un mediu dinamic bazat pe clasicul joc FrozenLake. Mediul a fost modificat pentru a include mai multe mecanici dinamice care cresc complexitatea È™i realismul problemei.

### Caracteristici Principale

- **Mediu Personalizat**: FrozenLake cu dificultate crescÃ¢ndÄƒ
- **3 Algoritmi RL**: Q-Learning (tabular), DQN (deep), PPO (policy-based)
- **Experimente Multiple**: 5 rulÄƒri independente per algoritm
- **AnalizÄƒ CompletÄƒ**: Grafice, tabele, metrici de performanÈ›Äƒ
- **Cod Bine Structurat**: Modular, comentat, extensibil

## ğŸ“ Structura Proiectului

```
proiect_irl/
â”‚
â”œâ”€â”€ environments/              # Mediul personalizat
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dynamic_frozenlake.py # Implementare DynamicFrozenLake
â”‚
â”œâ”€â”€ agents/                    # AgenÈ›ii RL
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ q_learning.py         # Q-Learning tabular
â”‚   â”œâ”€â”€ dqn.py                # Deep Q-Network
â”‚   â””â”€â”€ ppo.py                # Proximal Policy Optimization
â”‚
â”œâ”€â”€ experiments/               # Scripturi pentru experimente
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ run_experiments.py    # Rulare experimente
â”‚   â””â”€â”€ visualize.py          # Vizualizare rezultate
â”‚
â”œâ”€â”€ results/                   # Rezultate experimente
â”‚   â””â”€â”€ experiment_YYYYMMDD_HHMMSS/
â”‚       â”œâ”€â”€ results.json      # Date brute
â”‚       â””â”€â”€ plots/            # Grafice È™i tabele
â”‚
â”œâ”€â”€ .venv/                    # Virtual environment
â”œâ”€â”€ requirements.txt          # DependenÈ›e Python
â””â”€â”€ README.md                 # Acest fiÈ™ier
```

## ğŸš€ Instalare

### CerinÈ›e

- Python 3.8+
- pip

### PaÈ™i de Instalare

1. **Clonare/DescÄƒrcare Proiect**
   ```bash
   cd C:\Users\Horia\PyCharmMiscProject\proiect_irl
   ```

2. **Activare Virtual Environment**

   **Windows:**
   ```bash
   .venv\Scripts\activate
   ```

   **Linux/Mac:**
   ```bash
   source .venv/bin/activate
   ```

3. **Instalare DependenÈ›e**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ’» Utilizare

### Rulare Experimente

Pentru a antrena toÈ›i cei 3 agenÈ›i È™i a rula experimentele complete:

```bash
cd experiments
python run_experiments.py
```

Acest script va:
- Antrena Q-Learning pentru 500 episoade (5 rulÄƒri)
- Antrena DQN pentru 500 episoade (5 rulÄƒri)
- Antrena PPO pentru 50,000 timesteps (5 rulÄƒri)
- Salva rezultatele Ã®n `results/experiment_TIMESTAMP/`

### Vizualizare Rezultate

DupÄƒ rularea experimentelor, genereazÄƒ graficele È™i tabelele:

```bash
cd experiments
python visualize.py
```

Acest script va genera:
- `learning_curves.png` - Curbe de Ã®nvÄƒÈ›are pentru toÈ›i algoritmii
- `final_comparison.png` - ComparaÈ›ie metrici finale
- `convergence_analysis.png` - AnalizÄƒ convergenÈ›Äƒ È™i stabilitate
- `comparison_table.csv` - Tabel cu toate metricile

### Testare RapidÄƒ a Mediului

```python
from environments.dynamic_frozenlake import DynamicFrozenLakeEnv

# CreeazÄƒ mediul
env = DynamicFrozenLakeEnv(
    map_size=8,
    max_steps=100,
    render_mode="human"
)

# Test episod
state, _ = env.reset()
for _ in range(100):
    action = env.action_space.sample()  # AcÈ›iune aleatorie
    state, reward, terminated, truncated, info = env.step(action)
    env.render()

    if terminated or truncated:
        break

env.close()
```

## ğŸ® Arhitectura Mediului

### DynamicFrozenLake

Mediul `DynamicFrozenLakeEnv` extinde conceptul clasic FrozenLake cu urmÄƒtoarele mecanici dinamice:

#### Caracteristici Dinamice

1. **Probabilitate de Alunecare VariabilÄƒ**
   - PorneÈ™te de la 0.1 (10% È™ansÄƒ de alunecare)
   - CreÈ™te liniar pÃ¢nÄƒ la 0.4 (40% È™ansÄƒ)
   - CreÈ™te pe parcursul episodului â†’ dificultate crescÃ¢ndÄƒ

2. **Penalizare pentru PaÈ™i**
   - Fiecare pas costÄƒ -0.01 reward
   - ÃncurajeazÄƒ cÄƒi optime È™i eficiente

3. **GheaÈ›Äƒ care se TopeÈ™te**
   - GheaÈ›a se topeÈ™te progresiv (transformÃ¢ndu-se Ã®n gÄƒuri)
   - Rata de topire: 1% per pas
   - Face mediul mai imprevizibil

4. **Dimensiune VariabilÄƒ**
   - HÄƒrÈ›i de la 4x4 pÃ¢nÄƒ la 16x16
   - Proiectul foloseÈ™te 8x8 (64 stÄƒri)

5. **LimitÄƒ de PaÈ™i**
   - Maximum 100 paÈ™i per episod
   - Previne bucle infinite

#### SpaÈ›ii

- **Observation Space**: Discrete(64) - stÄƒri de la 0 la 63
- **Action Space**: Discrete(4) - LEFT, DOWN, RIGHT, UP

#### Rewards

- **Goal**: +1.0 (ajunge la destinaÈ›ie)
- **Hole**: 0.0 (cade Ã®n gaurÄƒ)
- **Step**: -0.01 (fiecare pas)

## ğŸ¤– Algoritmi ImplementaÈ›i

### 1. Q-Learning (Tabular)

**Tip**: Metoda tabularÄƒ clasicÄƒ
**Complexitate**: O(|S| Ã— |A|) = O(64 Ã— 4) = 256 intrÄƒri Ã®n tabelÄƒ

#### Principiu

Q-Learning Ã®nvaÈ›Äƒ o tabelÄƒ Q(s, a) care estimeazÄƒ reward-ul cumulativ aÈ™teptat pentru fiecare pereche (stare, acÈ›iune).

**Update Rule**:
```
Q(s, a) â† Q(s, a) + Î±[r + Î³ max Q(s', a') - Q(s, a)]
```

#### Hiperparametri

- Learning rate (Î±): 0.1
- Discount factor (Î³): 0.99
- Epsilon start: 1.0
- Epsilon end: 0.01
- Epsilon decay: 0.995

#### Avantaje

- Simplu È™i eficient pentru spaÈ›ii discrete mici
- GaranteazÄƒ convergenÈ›Äƒ la politica optimÄƒ
- Nu necesitÄƒ reÈ›ele neurale

#### Dezavantaje

- Nu scaleazÄƒ la spaÈ›ii mari de stÄƒri
- Nu poate generaliza Ã®ntre stÄƒri similare

---

### 2. DQN (Deep Q-Network)

**Tip**: Deep Reinforcement Learning (value-based)
**ArhitecturÄƒ**: MLP cu 2 straturi ascunse (128 neuroni fiecare)

#### Principiu

DQN foloseÈ™te o reÈ›ea neuronalÄƒ pentru a aproxima funcÈ›ia Q, permiÈ›Ã¢nd generalizare Ã®ntre stÄƒri.

**Componente Cheie**:
- **Experience Replay**: Buffer de 10,000 experienÈ›e
- **Target Network**: Actualizat la fiecare 10 episoade
- **Epsilon-Greedy**: Explorare vs. exploatare

#### ArhitecturÄƒ ReÈ›ea

```
Input (64) â†’ Dense(128) â†’ ReLU â†’ Dense(128) â†’ ReLU â†’ Output(4)
```

#### Hiperparametri

- Learning rate: 0.001
- Discount factor (Î³): 0.99
- Batch size: 64
- Buffer capacity: 10,000
- Target update frequency: 10 episoade
- Hidden dim: 128

#### Avantaje

- ScaleazÄƒ la spaÈ›ii mari de stÄƒri
- GeneralizeazÄƒ Ã®ntre stÄƒri similare
- Poate Ã®nvÄƒÈ›a din experienÈ›e anterioare

#### Dezavantaje

- Mai complex decÃ¢t Q-Learning
- NecesitÄƒ tuning atent al hiperparametrilor
- Poate fi instabil fÄƒrÄƒ experience replay

---

### 3. PPO (Proximal Policy Optimization)

**Tip**: Policy-based (modern policy gradient)
**Implementare**: Stable Baselines3

#### Principiu

PPO Ã®nvaÈ›Äƒ direct o politicÄƒ (mapare stare â†’ acÈ›iune) Ã®n loc de o funcÈ›ie Q.

**Caracteristici**:
- **Clipped Objective**: Previne update-uri prea mari
- **GAE**: Generalized Advantage Estimation
- **Multiple Epochs**: ÃnvaÈ›Äƒ din acelaÈ™i batch de date

#### Hiperparametri

- Learning rate: 0.0003
- N steps: 2,048
- Batch size: 64
- N epochs: 10
- Gamma: 0.99
- GAE lambda: 0.95
- Clip range: 0.2

#### Avantaje

- Foarte stabil È™i robust
- State-of-the-art pentru multe task-uri
- FuncÈ›ioneazÄƒ bine out-of-the-box

#### Dezavantaje

- Mai lent decÃ¢t DQN (necesitÄƒ mai multe sample-uri)
- Hiperparametri mai complecÈ™i

## ğŸ“Š Rezultate

### Metrici de Evaluare

Pentru fiecare algoritm se mÄƒsoarÄƒ:

1. **Mean Reward**: Reward-ul mediu pe episod
2. **Success Rate**: Procentul de episoade finalizate cu succes
3. **Mean Steps**: NumÄƒrul mediu de paÈ™i pÃ¢nÄƒ la terminare
4. **Convergence**: Viteza de convergenÈ›Äƒ cÄƒtre politica optimÄƒ
5. **Stability**: VariaÈ›ia Ã®n performanÈ›Äƒ (stabilitate)

### Rezultate AÈ™teptate

**Ierarhie AÈ™teptatÄƒ** (de la cel mai bun la cel mai slab):

1. **PPO**: Cel mai bun success rate È™i stabilitate
2. **DQN**: PerformanÈ›Äƒ bunÄƒ, dar mai instabil
3. **Q-Learning**: PerformanÈ›Äƒ decentÄƒ, dar mai lent

### Grafice Generate

1. **Learning Curves**
   - Reward per episode
   - Steps per episode
   - Epsilon decay
   - Loss (DQN)

2. **Final Comparison**
   - Mean reward cu standard deviation
   - Success rate cu standard deviation

3. **Convergence Analysis**
   - Rolling average reward
   - Variance Ã®n reward (stabilitate)

## ğŸ”§ Probleme ÃntÃ¢mpinate È™i SoluÈ›ii

### 1. Instabilitate DQN

**ProblemÄƒ**: DQN avea performanÈ›Äƒ instabilÄƒ Ã®n primele episoade.

**CauzÄƒ**:
- Replay buffer gol la Ã®nceput
- Target network nu era actualizat suficient de des

**SoluÈ›ie**:
- Crescut dimensiunea buffer-ului la 10,000
- Optimizat frecvenÈ›a de actualizare a target network
- AdÄƒugat warm-up period pentru replay buffer

### 2. Explorare InsuficientÄƒ Q-Learning

**ProblemÄƒ**: Q-Learning converge prematur cÄƒtre politici suboptimale.

**CauzÄƒ**:
- Epsilon decay prea rapid
- Nu exploreazÄƒ suficient spaÈ›iul de stÄƒri

**SoluÈ›ie**:
- Ajustat epsilon decay de la 0.99 la 0.995
- Crescut numÄƒrul de episoade de antrenament

### 3. Mediu Prea Dificil

**ProblemÄƒ**: ToÈ›i algoritmii aveau success rate < 10% iniÈ›ial.

**CauzÄƒ**:
- Probabilitate de alunecare prea mare
- Topirea gheÈ›ii prea rapidÄƒ

**SoluÈ›ie**:
- Redus slippery_start de la 0.3 la 0.1
- Redus melting_rate de la 0.02 la 0.01
- Ajustat step_penalty pentru a nu penaliza prea mult

### 4. PPO Lent

**ProblemÄƒ**: PPO necesitÄƒ mult timp pentru antrenament.

**CauzÄƒ**:
- N_steps prea mare (4096)
- Multiple epochs per batch

**SoluÈ›ie**:
- Redus n_steps la 2048
- Optimizat batch_size la 64
- Folosit vectorized environments (posibil upgrade viitor)

## ğŸ“ ÃnvÄƒÈ›Äƒminte

### Concluzii Tehnice

1. **Q-Learning** funcÈ›ioneazÄƒ bine pentru medii simple È™i discrete
2. **DQN** oferÄƒ flexibilitate dar necesitÄƒ tuning atent
3. **PPO** este cel mai robust dar È™i cel mai costisitor

### Best Practices

- Ãncepe cu metode simple (Q-Learning) Ã®nainte de deep RL
- FoloseÈ™te experimente multiple pentru a evalua stabilitatea
- MonitorizeazÄƒ nu doar reward-ul, ci È™i alte metrici (steps, success rate)
- Vizualizarea rezultatelor este esenÈ›ialÄƒ pentru Ã®nÈ›elegere

## ğŸ“š ReferinÈ›e

- [Sutton & Barto - Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html)
- [DQN Paper (Mnih et al., 2015)](https://www.nature.com/articles/nature14236)
- [PPO Paper (Schulman et al., 2017)](https://arxiv.org/abs/1707.06347)
- [Stable Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
- [Gymnasium Documentation](https://gymnasium.farama.org/)

## ğŸ‘¥ Autor

Proiect realizat pentru cursul de Introducere Ã®n Reinforcement Learning (IRL).

## ğŸ“ LicenÈ›Äƒ

Acest proiect este realizat Ã®n scop educaÈ›ional.
