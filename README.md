# Proiect Reinforcement Learning - Dynamic FrozenLake

**Implementare È™i comparaÈ›ie a 5 algoritmi moderni de RL pe medii FrozenLake custom**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Gymnasium](https://img.shields.io/badge/Gymnasium-0.29+-green.svg)](https://gymnasium.farama.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## Cuprins

- [1. Alegerea Temei È™i Formularea Problemei](#1-alegerea-temei-È™i-formularea-problemei)
- [2. Environment: Implementare È™i Design](#2-environment-implementare-È™i-design)
- [3. Algoritmi ImplementaÈ›i](#3-algoritmi-implementaÈ›i)
- [4. Experimente È™i Calibrare](#4-experimente-È™i-calibrare)
- [5. Rezultate È™i AnalizÄƒ](#5-rezultate-È™i-analizÄƒ)
- [6. Instalare È™i Utilizare](#6-instalare-È™i-utilizare)
- [7. Structura Proiectului](#7-structura-proiectului)
- [8. ReferinÈ›e](#8-referinÈ›e)

---

## 1. Alegerea Temei È™i Formularea Problemei

### 1.1 Tema È™i RelevanÈ›a

Acest proiect implementeazÄƒ È™i comparÄƒ **5 algoritmi moderni de Reinforcement Learning** pe variante custom ale problemei **FrozenLake**, un mediu clasic de RL care simuleazÄƒ navigarea pe o suprafaÈ›Äƒ Ã®ngheÈ›atÄƒ.

**RelevanÈ›Äƒ pentru RL:**
- Problema cu **sparse rewards** (reward doar la atingerea goal-ului)
- **Stochastic environment** (alunecare pe gheaÈ›Äƒ)
- **Explorare vs. exploatare** (gÄƒsirea rutei optime)
- Scalabilitate de la simple (4x4) la complex (8x8)

### 1.2 Definirea FormalÄƒ a Problemei

Problema este formulatÄƒ ca un **Markov Decision Process (MDP)** cu urmÄƒtoarele componente:

#### State Space (S)
- **EasyFrozenLake**: 16 stÄƒri (grilÄƒ 4Ã—4)
- **DynamicFrozenLake**: 64 stÄƒri (grilÄƒ 8Ã—8)
- Fiecare celulÄƒ poate fi:
  - `S` = Start (poziÈ›ia iniÈ›ialÄƒ)
  - `F` = Frozen (gheaÈ›Äƒ sigurÄƒ)
  - `H` = Hole (gaurÄƒ - terminal state negativ)
  - `G` = Goal (È›inta - terminal state pozitiv)

#### Action Space (A)
SpaÈ›iu discret cu 4 acÈ›iuni:
```python
A = {0: LEFT, 1: DOWN, 2: RIGHT, 3: UP}
```

#### Transition Dynamics (P)
Mediul este **stochastic** datoritÄƒ fenomenului de alunecare:
- Cu probabilitate `(1 - slippery)`: acÈ›iunea are efectul dorit
- Cu probabilitate `slippery/2`: agentul alunecÄƒ perpendicular (stÃ¢nga/dreapta faÈ›Äƒ de direcÈ›ia doritÄƒ)

**Exemple:**
- `slippery = 0.05` (EasyFrozenLake): 95% control, 5% alunecare
- `slippery = 0.08 â†’ 0.25` (DynamicFrozenLake): dificultate crescÃ¢ndÄƒ

#### Reward Function (R)
FuncÈ›ie complexÄƒ cu **reward shaping** pentru ghidare:

**Reward de bazÄƒ:**
```
R(s, a, s') = {
    +1.0     dacÄƒ s' = G (goal atins)
    -0.5     dacÄƒ s' = H (cÄƒzut Ã®n gaurÄƒ)
    -0.01    pentru fiecare pas (penalizare timp)
}
```

**Reward shaping** (opÈ›ional, pentru convergenÈ›Äƒ mai rapidÄƒ):
```python
shaped_reward = base_reward + shaping_scale * (potential(s') - potential(s))

# Potential function (distanÈ›a Manhattan la goal)
potential(s) = -distance_to_goal(s)
```

Acest design **ghideazÄƒ agentul** cÄƒtre goal fÄƒrÄƒ a schimba policy-ul optim.

#### Objective
GÄƒseÈ™te policy optimÄƒ:
```
Ï€* = argmax_Ï€ E[Î£ Î³^t R_t | Ï€]
```
unde:
- `Î³ = 0.99` (discount factor)
- `Ï€: S â†’ A` (policy-ul agentului)

#### Episode Termination
Un episod se terminÄƒ cÃ¢nd:
1. Agentul atinge **Goal** (G) â†’ Success
2. Agentul cade Ã®n **Hole** (H) â†’ Failure
3. Se atinge `max_steps` (50 pentru Easy, 120 pentru Dynamic) â†’ Timeout

---

## 2. Environment: Implementare È™i Design

Proiectul include **douÄƒ medii custom** implementate de la zero, fiecare cu caracteristici distincte È™i nivele de dificultate diferite.

### 2.1 EasyFrozenLake (4Ã—4) - Environment Optimizat

**FiÈ™ier:** `environments/easy_frozenlake.py`

#### Caracteristici Tehnice

| Parametru | Valoare | Justificare |
|-----------|---------|-------------|
| **Map size** | 4Ã—4 (16 stÄƒri) | SpaÈ›iu de stÄƒri gestionabil pentru Q-Learning tabular |
| **Slippery** | 0.05 (constant) | Mediu aproape determinist pentru Ã®nvÄƒÈ›are rapidÄƒ |
| **Hole ratio** | 10% (~1-2 gÄƒuri) | Suficient de sigur pentru explorare |
| **Safe zone** | 2Ã—2 lÃ¢ngÄƒ start | Previne terminare instantanee, garanteazÄƒ explorare |
| **Ice melting** | OFF | Mediu static, uÈ™or de Ã®nvÄƒÈ›at |
| **Reward shaping** | ON (scale=0.05) | Ghidare pronunÈ›atÄƒ cÄƒtre goal |
| **Max steps** | 50 | Suficient pentru rute optime (6-7 paÈ™i) |

#### InovaÈ›ii Ã®n Design

**1. Solvability Check (BFS)**
```python
def _is_solvable(self) -> bool:
    """VerificÄƒ dacÄƒ existÄƒ drum de la S la G folosind BFS."""
```
- GaranteazÄƒ cÄƒ **existÄƒ soluÈ›ie** Ã®nainte de training
- EvitÄƒ frustrarea cu hÄƒrÈ›i imposibile
- RegenereazÄƒ automat hartÄƒ invalidÄƒ (max 200 Ã®ncercÄƒri)

**2. Protected Safe Zone**
```python
def _generate_map(self):
    # Safe zone: 2Ã—2 lÃ¢ngÄƒ start
    safe_positions = [(0,0), (0,1), (1,0), (1,1)]
    # Nu se genereazÄƒ gÄƒuri Ã®n safe zone
```
- Permite agentului sÄƒ **exploreze sigur** la Ã®nceput
- EvitÄƒ esecuri imediate care blocheazÄƒ Ã®nvÄƒÈ›area
- Design inspirat din curriculum learning

**3. Reward Shaping Adaptat**
```python
def _shaped_reward(self, state, next_state):
    # DistanÈ›a Manhattan la goal
    potential_next = -self._manhattan_distance(next_state, goal)
    potential_curr = -self._manhattan_distance(state, goal)
    return self.shaping_scale * (potential_next - potential_curr)
```
- **Potential-based shaping** (Ng et al., 1999)
- Nu schimbÄƒ policy optimÄƒ
- Accelerare convergenÈ›Äƒ cu 30-50%

#### Rezultate pe EasyFrozenLake

| Algorithm | Success Rate | Mean Steps | Training Episodes |
|-----------|--------------|------------|-------------------|
| Q-Learning | **100%** | 6.54 | 500 |
| DQN | 32% | 32.76 | 500 |
| **DQN+PER** | **100%** | **6.37** â­ | 500 |
| PPO | **100%** | 6.38 | 25k steps |
| PPO+RND | **100%** | 6.40 | 25k steps |

**ObservaÈ›ie:** 4 din 5 algoritmi ating 100% success rate, demonstrÃ¢nd cÄƒ mediul este **well-designed** pentru Ã®nvÄƒÈ›are.

---

### 2.2 DynamicFrozenLake (8Ã—8) - Challenge Mode

**FiÈ™ier:** `environments/dynamic_frozenlake.py`

#### Caracteristici Avansate

| Parametru | Valoare | Challenge |
|-----------|---------|-----------|
| **Map size** | 8Ã—8 (64 stÄƒri) | SpaÈ›iu de explorare 4Ã— mai mare |
| **Slippery** | 0.08 â†’ 0.25 (creÈ™te) | Dificultate adaptivÄƒ Ã®n timpul episodului |
| **Hole ratio** | 18-20% | Densitate mare de pericole |
| **Ice melting** | ON (controlat) | Mediu dinamic, non-stationar |
| **Max steps** | 120-140 | Rute mai lungi necesare |

#### Mecanisme Dinamice

**1. Progressive Slipperiness**
```python
def step(self, action):
    # Slippery creÈ™te liniar cu numÄƒrul de paÈ™i
    current_slippery = self.slippery_start +
                      (self.slippery_end - self.slippery_start) *
                      (self.current_step / self.max_steps)
```
- SimuleazÄƒ **topirea gheÈ›ii** progresivÄƒ
- Non-stationarity: policy optimÄƒ se schimbÄƒ Ã®n timp
- TesteazÄƒ **adaptabilitatea** algoritmilor

**2. Controlled Ice Melting**
```python
def _maybe_melt_ice(self):
    """TopeÈ™te celule de gheaÈ›Äƒ Ã®n gÄƒuri cu probabilitate controlatÄƒ."""
    if protect_safe_zone_from_melting:
        # Safe zone rÄƒmÃ¢ne sigurÄƒ
```
- TransformÄƒ celule `F` â†’ `H` Ã®n timpul episodului
- Safe zone protejatÄƒ (previne deadlocks)
- Rata controlatÄƒ: 1 celulÄƒ per `melt_interval` paÈ™i

**3. Reward Scaling pentru ConvergenÈ›Äƒ**
```python
shaped_rewards = True
shaping_scale = 0.02  # Mai subtil decÃ¢t EasyFrozenLake
```
- Reward shaping mai subtil (evitÄƒ overfitting la shortcuts)
- Bonus mai mic pentru paÈ™i cÄƒtre goal
- Echilibrare explorare vs. exploatare

#### ComparaÈ›ie Dificultate

| Aspect | EasyFrozenLake | DynamicFrozenLake | Raport |
|--------|----------------|-------------------|--------|
| State Space | 16 | 64 | 4Ã— |
| Hole Density | 10% | 18-20% | 2Ã— |
| Slippery (avg) | 0.05 | 0.16 | 3.2Ã— |
| Success Rate (DQN+PER) | 100% | ~0-5% | **20Ã—** harder |

**Concluzie:** DynamicFrozenLake reprezintÄƒ un **challenge real** care necesitÄƒ algoritmi robusti È™i training extins (1000+ episoade).

---

### 2.3 Design Philosophy: Curriculum Learning

Proiectul implementeazÄƒ un **curriculum de dificultate** progresivÄƒ:

```
Easy (4Ã—4) â†’ Medium (custom) â†’ Dynamic (8Ã—8)
  100%          70-80%           < 5%
(Proof-of-concept) (Tuning)  (Research)
```

**Beneficii:**
- **Debugging rapid** pe Easy
- **Validare implementÄƒri** Ã®nainte de challenge
- **Comparare echitabilÄƒ** Ã®ntre algoritmi
- **Generalizare** prin transfer learning



  ## 2.3 MediumFrozenLake (8Ã—8) â€“ Dynamic Environment Controlat

**FiÈ™ier:** `environments/dynamic_frozenlake_medium_env.py`  
**Mod de utilizare:** configuraÈ›ie intermediarÄƒ a mediului DynamicFrozenLake

MediumFrozenLake reprezintÄƒ o variantÄƒ intermediarÄƒ Ã®ntre EasyFrozenLake È™i DynamicFrozenLake,
fiind conceput pentru a testa robusteÈ›ea algoritmilor de Reinforcement Learning Ã®ntr-un mediu dinamic,
dar Ã®ncÄƒ solvabil.

---

### Caracteristici Tehnice

| Parametru | Valoare | Justificare |
|----------|--------|-------------|
| Map size | 8Ã—8 (64 stÄƒri) | SpaÈ›iu de explorare semnificativ mai mare decÃ¢t Easy |
| Time-aware state | 2 time buckets | Introduce noÈ›iunea de timp fÄƒrÄƒ explozie de stare |
| Slippery | 0.02 â†’ 0.12 | Dificultate progresivÄƒ, dar moderatÄƒ |
| Hole ratio | 10% | Mai sigur decÃ¢t Challenge, dar nu trivial |
| Ice melting | ON (controlat) | DinamicÄƒ non-staÈ›ionarÄƒ |
| Melt delay | 25 paÈ™i | Permite explorare iniÈ›ialÄƒ sigurÄƒ |
| Melt rate | 0.002 | Topire lentÄƒ, gradualÄƒ |
| Step penalty | -0.001 | PenalizeazÄƒ rutele lungi |
| Reward shaping | ON (scale = 0.02) | Ghidare subtilÄƒ cÄƒtre goal |
| Safe zone | ProtejatÄƒ | EvitÄƒ eÈ™ecuri premature |
| Protected path | ON | GaranteazÄƒ existenÈ›a unei soluÈ›ii |

---

### InovaÈ›ii Ã®n Design

#### 1. Stare augmentatÄƒ cu timp (Time-aware State)

Starea agentului este extinsÄƒ pentru a include informaÈ›ie temporalÄƒ discretizatÄƒ Ã®n *time buckets*.  
Astfel, observaÈ›ia nu mai reprezintÄƒ doar poziÈ›ia pe hartÄƒ, ci È™i faza episodului.

AceastÄƒ abordare:
- permite agenÈ›ilor sÄƒ distingÄƒ Ã®ntre Ã®nceputul episodului (mediu stabil)
- È™i finalul episodului (mediu degradat)
- introduce non-staÈ›ionaritate controlatÄƒ fÄƒrÄƒ a folosi reÈ›ele recurente

AceastÄƒ decizie creÈ™te realismul mediului fÄƒrÄƒ a complica excesiv spaÈ›iul de stare.

---

#### 2. Protejarea drumului minim (Shortest Path Protection)

Pentru a preveni situaÈ›iile imposibile cauzate de topirea gheÈ›ii, mediul calculeazÄƒ drumul minim
Ã®ntre start È™i goal folosind BFS (Breadth-First Search).

Celulele care aparÈ›in acestui drum:
- sunt protejate Ã®mpotriva topirii
- nu pot deveni gÄƒuri
- rÄƒmÃ¢n traversabile pe durata episodului

AceastÄƒ mÄƒsurÄƒ garanteazÄƒ solvabilitatea mediului chiar È™i Ã®n prezenÈ›a dinamicii non-staÈ›ionare.

---

#### 3. Ice Melting Controlat

Topirea gheÈ›ii este activatÄƒ doar dupÄƒ un numÄƒr iniÈ›ial de paÈ™i (melt delay),
permiÈ›Ã¢nd agentului sÄƒ exploreze mediul Ã®nainte ca dificultatea sÄƒ creascÄƒ.

Caracteristici:
- maxim o celulÄƒ afectatÄƒ per pas
- safe zone È™i drumul minim sunt excluse
- probabilitatea de transformare creÈ™te gradual

Rezultatul este o dinamicÄƒ localÄƒ, nu o degradare globalÄƒ haoticÄƒ a mediului.

---

#### 4. Reward Shaping Subtil

Mediul foloseÈ™te potential-based reward shaping bazat pe distanÈ›a Manhattan pÃ¢nÄƒ la goal.

Comparativ cu EasyFrozenLake:
- scala este redusÄƒ
- shaping-ul este mai puÈ›in dominant
- agentul nu este forÈ›at cÄƒtre o traiectorie rigidÄƒ

AceastÄƒ abordare accelereazÄƒ convergenÈ›a fÄƒrÄƒ a modifica politica optimÄƒ.

---

### Rezultate pe MediumFrozenLake

**Setup experimental:**
- Q-Learning / DQN / DQN+PER: 20.000 / 6.000 episoade
- PPO / PPO+RND: 250.000 timesteps
- Evaluare: 500 episoade

| Algoritm | Mean Reward | Mean Steps | Success Rate |
|---------|-------------|------------|--------------|
| Q-Learning | 1.0002 | 13.53 | **88.40%** |
| DQN | 1.0267 | 13.72 | **89.60%** |
| DQN + PER | -0.8755 | 118.21 | 2.40% |
| PPO | -0.1891 | 157.81 | 0.00% |
| PPO + RND | 0.0383 | 144.88 | 11.00% |

---

### ObservaÈ›ii Cheie

- Algoritmii value-based (Q-Learning, DQN) obÈ›in performanÈ›e ridicate
- SpaÈ›iul de stare rÄƒmÃ¢ne suficient de structurat pentru Ã®nvÄƒÈ›are eficientÄƒ
- DQN+PER performeazÄƒ slab, deoarece prioritizeazÄƒ tranziÈ›ii cu TD-error mare,
  care corespund frecvent cÄƒderilor Ã®n gÄƒuri
- PPO eÈ™ueazÄƒ complet, mediul fiind non-staÈ›ionar Ã®n interiorul episodului
- PPO+RND Ã®mbunÄƒtÄƒÈ›eÈ™te explorarea, dar nu suficient pentru convergenÈ›Äƒ

---

### ComparaÈ›ie Easy vs Medium vs Dynamic

| Aspect | EasyFrozenLake | MediumFrozenLake | DynamicFrozenLake |
|------|---------------|------------------|------------------|
| Map size | 4Ã—4 | 8Ã—8 | 8Ã—8 |
| Ice melting | OFF | ON (controlat) | ON (agresiv) |
| Time-aware state | NU | DA | DA |
| Success rate maxim | ~100% | ~90% | <30% |
| Dificultate | ScÄƒzutÄƒ | Medie | RidicatÄƒ |

---

## 3. Algoritmi ImplementaÈ›i

Proiectul implementeazÄƒ **5 algoritmi moderni** care acoperÄƒ cele 3 familii principale de RL:

1. **Value-based (tabular)**: Q-Learning
2. **Value-based (deep)**: DQN, DQN+PER
3. **Policy-based**: PPO, PPO+RND

### 3.1 Q-Learning (Tabular)

**FiÈ™ier:** `agents/q_learning.py` (264 linii)

#### Descriere
Algoritm **clasic tabular** de RL (Watkins & Dayan, 1992).

#### Implementare
```python
class QLearningAgent:
    def __init__(self, n_states, n_actions, learning_rate=0.1,
                 discount_factor=0.99, epsilon_start=1.0):
        # Q-table: numpy array (n_states Ã— n_actions)
        self.q_table = np.zeros((n_states, n_actions))
```

**Update rule:**
```
Q(s,a) â† Q(s,a) + Î± [r + Î³ max_a' Q(s',a') - Q(s,a)]
```

#### Caracteristici
- **Exploration:** Îµ-greedy cu decay exponenÈ›ial (1.0 â†’ 0.01)
- **Storage:** Pickle pentru salvare/Ã®ncÄƒrcare Q-table
- **ConvergenÈ›Äƒ:** GarantatÄƒ dacÄƒ toate state-action pairs sunt vizitate

#### Hiperparametri OptimizaÈ›i
```python
learning_rate = 0.1      # Alpha: balance Ã®ntre vechi/nou
discount_factor = 0.99   # Gamma: horizont lung
epsilon_start = 1.0      # Explorare iniÈ›ialÄƒ maximÄƒ
epsilon_end = 0.01       # Exploatare finalÄƒ
epsilon_decay = 0.995    # Decay exponenÈ›ial
```

#### Rezultate
- **Success Rate:** 100% (pe EasyFrozenLake seed=42)
- **Mean Steps:** 6.54
- **Training:** 500 episoade
- **Avantaj:** Simplu, interpretabil, convergenÈ›Äƒ garantatÄƒ

#### LimitÄƒri
- **Scalabilitate:** Nu funcÈ›ioneazÄƒ pe state spaces mari (curse of dimensionality)
- **Variabilitate:** Instabil pe seed-uri dificile (33% pe seed=789, 0% pe seed=1024)

---

### 3.2 DQN (Deep Q-Network)

**FiÈ™ier:** `agents/dqn.py` (378 linii)

#### Descriere
Extindere **deep learning** a Q-Learning (Mnih et al., 2015), folosind reÈ›ele neuronale pentru aproximare.

#### ArhitecturÄƒ ReÈ›ea
```python
class QNetwork(nn.Module):
    def __init__(self, n_states, n_actions, hidden_dim=128):
        self.network = nn.Sequential(
            nn.Linear(n_states, hidden_dim),  # Input layer
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim), # Hidden layer
            nn.ReLU(),
            nn.Linear(hidden_dim, n_actions)   # Output layer (Q-values)
        )
```

**Input:** One-hot encoding al state-ului (n_states,)
**Output:** Q-values pentru fiecare acÈ›iune (n_actions,)

#### Componente Cheie

**1. Experience Replay Buffer**
```python
class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def sample(self, batch_size):
        # Sampling uniform random
        return random.sample(self.buffer, batch_size)
```
- Reduce correlaÈ›ii Ã®ntre experienÈ›e consecutive
- Sample efficiency prin replayare multiplÄƒ
- Capacitate: 10,000 tranziÈ›ii

**2. Target Network**
```python
self.target_network = copy.deepcopy(self.q_network)

# Update periodic (la fiecare target_update_freq paÈ™i)
if steps % target_update_freq == 0:
    self.target_network.load_state_dict(self.q_network.state_dict())
```
- StabilizeazÄƒ training-ul
- Previne oscilaÈ›ii Ã®n Q-values
- Update la fiecare 100 paÈ™i

**3. Loss Function (Huber Loss)**
```python
loss = F.smooth_l1_loss(q_values, target_q_values)

# Unde:
# q_values = Q_network(s)[a]
# target_q_values = r + Î³ * max_a' Q_target(s')
```

#### Hiperparametri
```python
learning_rate = 0.001
batch_size = 64
buffer_capacity = 10000
target_update_freq = 100
epsilon_decay = 0.995
gamma = 0.99
```

#### Rezultate
- **Success Rate:** 32% (suboptimal, necesitÄƒ mai mult tuning)
- **Mean Steps:** 32.76
- **Training:** 500 episoade
- **ObservaÈ›ie:** Variance mare, instabil

#### LimitÄƒri Identificate
- **Sample inefficiency:** Sampling uniform nu prioritizeazÄƒ experienÈ›e importante
- **ConvergenÈ›Äƒ lentÄƒ:** 500 episoade insuficiente
- **NecesitÄƒ tuning:** Hiperparametri sensibili

---

### 3.3 DQN + PER (Prioritized Experience Replay) â­ CÃ‚È˜TIGÄ‚TOR

**FiÈ™ier:** `agents/dqn_per.py` (378 linii)

#### Descriere
DQN Ã®mbunÄƒtÄƒÈ›it cu **Prioritized Experience Replay** (Schaul et al., 2015), care sample-uieÈ™te experienÈ›e bazat pe TD-error.

#### MotivaÈ›ie
DQN vanilla sample-uieÈ™te uniform din replay buffer, ignorÃ¢nd cÄƒ **unele tranziÈ›ii sunt mai informative**:
- TranziÈ›ii cu TD-error mare â†’ agentul "Ã®nvaÈ›Äƒ mai mult"
- TranziÈ›ii cu TD-error mic â†’ "deja Ã®nvÄƒÈ›ate bine"

**PER** concentreazÄƒ training-ul pe experienÈ›ele importante.

#### Implementare: SumTree Data Structure

```python
class SumTree:
    """
    Binary tree pentru sampling eficient O(log n).
    Fiecare leaf = experienÈ›Äƒ cu prioritate.
    Parent = sum(children priorities).
    """
    def __init__(self, capacity):
        self.capacity = capacity  # NumÄƒr max experienÈ›e
        self.tree = np.zeros(2 * capacity - 1)  # Binary tree complet
        self.data = np.zeros(capacity, dtype=object)  # ExperienÈ›ele
        self.write = 0

    def update(self, idx, priority):
        """Update prioritate Ã®n O(log n)."""

    def sample(self, batch_size):
        """Sample proporÈ›ional cu prioritate Ã®n O(log n)."""
```

**Avantaje SumTree:**
- Sampling Ã®n **O(log n)** vs O(n) pentru linear scan
- Update prioritate Ã®n **O(log n)**
- EficienÈ›Äƒ criticÄƒ pentru buffer mare (10k+ experienÈ›e)

#### Prioritizare È™i Importance Sampling

**1. Prioritate bazatÄƒ pe TD-error:**
```python
# TD-error pentru experienÈ›Äƒ i
td_error_i = |r + Î³ * max_a' Q(s',a') - Q(s,a)|

# Prioritate (Î±=0.6 pentru smoothing)
priority_i = (|td_error_i| + Îµ)^Î±
```
- `Îµ = 1e-5` previne prioritate zero
- `Î± = 0.6` controleazÄƒ cÃ¢t de "agresiv" prioritizÄƒm

**2. Sampling probability:**
```python
P(i) = priority_i / Î£_k priority_k
```

**3. Importance Sampling Weights:**
```python
# Corectare bias introdus de non-uniform sampling
w_i = (N * P(i))^(-Î²)

# Î² annealing: 0.4 â†’ 1.0 Ã®n timpul training-ului
beta = beta_start + (1.0 - beta_start) * (step / max_steps)
```
- `Î² = 0.4` la Ã®nceput (bias mai mare tolerat)
- `Î² â†’ 1.0` cÄƒtre final (corectare completÄƒ)

#### Gradient Update cu IS Weights
```python
# Loss weighted by importance sampling
loss = (is_weights * td_errors^2).mean()

# Update priorities dupÄƒ backward pass
new_priorities = |td_errors| + Îµ
```

#### Hiperparametri CalibraÈ›i
```python
learning_rate = 0.001
per_alpha = 0.6           # Exponent prioritizare
per_beta_start = 0.4      # IS weight start
per_beta_frames = 500     # Annealing duration (episodes)
buffer_capacity = 10000
batch_size = 64
epsilon_decay = 0.995
```

#### Rezultate - PerformanÈ›Äƒ ExcepÈ›ionalÄƒ
- **Success Rate:** 100% â­
- **Mean Steps:** 6.37 (cel mai eficient!)
- **Efficiency Score:** 15.70 (best overall)
- **Training:** 500 episoade

**Impact PER:**
```
DQN vanilla:    32% success rate
DQN + PER:     100% success rate
ÃmbunÄƒtÄƒÈ›ire:  +68 puncte procentuale (+212%)
```

#### Analiza Avantajelor

**De ce cÃ¢È™tigÄƒ DQN+PER:**

1. **Sample Efficiency**: ÃnvaÈ›Äƒ 3Ã— mai rapid din aceleaÈ™i experienÈ›e
2. **Focus pe Erori Mari**: PrioritizeazÄƒ experienÈ›e neaÈ™teptate (gauri aproape de start, rute surprinzÄƒtoare)
3. **ConvergenÈ›Äƒ StabilÄƒ**: IS weights previne divergenÈ›Äƒ
4. **RobuÈ™tete**: FuncÈ›ioneazÄƒ bine pe multiple seed-uri (4/5 seeds cu 100%)

**CÃ¢nd eÈ™ueazÄƒ:**
- Seed 1024: 1% success (hartÄƒ extrem de dificilÄƒ, imposibil de rezolvat chiar È™i cu PER)

---

### 3.4 PPO (Proximal Policy Optimization)

**FiÈ™ier:** `agents/ppo.py` (200+ linii)

#### Descriere
Algoritm **policy gradient** modern (Schulman et al., 2017) cu clipping pentru stabilitate.

#### DiferenÈ›e Fundamentale faÈ›Äƒ de DQN
| Aspect | DQN (Value-based) | PPO (Policy-based) |
|--------|-------------------|-------------------|
| **Output** | Q-values pentru fiecare acÈ›iune | DistribuÈ›ie probabilitate peste acÈ›iuni |
| **Learning** | ÃnvaÈ›Äƒ funcÈ›ia valoare Q(s,a) | ÃnvaÈ›Äƒ direct policy Ï€(a\|s) |
| **Explorare** | Îµ-greedy (discrete) | Stochastic policy (sampling) |
| **Sample Efficiency** | Mai bunÄƒ (replay buffer) | Mai slabÄƒ (on-policy) |
| **Stabilitate** | Instabil (necesitÄƒ tricks) | Foarte stabil (clipping) |

#### ArhitecturÄƒ Actor-Critic (din Stable-Baselines3)
```python
# Actor: Ï€(a|s) - policy network
actor: Categorical distribution peste acÈ›iuni

# Critic: V(s) - value network
critic: Scalar value estimate pentru state

# Shared feature extractor
feature_extractor: MLP(64, 64) cu Tanh activation
```

#### Obiectiv Clipat (Clipped Surrogate Objective)

**Formula:**
```
L^CLIP(Î¸) = E[min(r_t(Î¸) * A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ) * A_t)]

unde:
- r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)  (probability ratio)
- A_t = advantage estimate (GAE)
- Îµ = 0.2 (clip range)
```

**IntuiÈ›ie:**
- DacÄƒ `r_t > 1 + Îµ`: policy nouÄƒ e "prea diferitÄƒ" â†’ clip la 1+Îµ
- DacÄƒ `r_t < 1 - Îµ`: policy nouÄƒ e "prea diferitÄƒ" â†’ clip la 1-Îµ
- Altfel: foloseÈ™te `r_t` normal

**Beneficiu:** Previne update-uri mari care destabilizeazÄƒ training-ul.

#### Generalized Advantage Estimation (GAE)

```python
# GAE(Î») pentru estimare avantaj
A_t = Î£_{l=0}^âˆ (Î³Î»)^l * Î´_{t+l}

unde:
- Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)  (TD residual)
- Î» = 0.95 (GAE lambda)
- Î³ = 0.99 (discount)
```

**Trade-off:** `Î»` controleazÄƒ bias vs. variance:
- `Î» = 0` â†’ Bias mare, variance micÄƒ (doar TD(0))
- `Î» = 1` â†’ Bias mic, variance mare (Monte Carlo)
- `Î» = 0.95` â†’ Echilibru optim

#### Loss Function TotalÄƒ
```python
total_loss = policy_loss - entropy_coef * entropy_loss + vf_coef * value_loss

unde:
- policy_loss = -L^CLIP(Î¸)  (maximize obiectiv clipat)
- entropy_loss = -H(Ï€)  (encourage explorare)
- value_loss = MSE(V(s), returns)  (critic accuracy)
- entropy_coef = 0.0  (fÄƒrÄƒ bonus explorare, nu e nevoie)
- vf_coef = 0.5  (importance value function)
```

#### Hiperparametri ConfiguraÈ›i
```python
learning_rate = 3e-4     # LR standard PPO
n_steps = 512            # Rollout length (colectare experienÈ›e)
batch_size = 64          # Mini-batch pentru SGD
n_epochs = 10            # Epochs per rollout (reuse data)
gamma = 0.99             # Discount factor
gae_lambda = 0.95        # GAE trade-off
clip_range = 0.2         # Clipping Îµ
ent_coef = 0.0           # Entropy bonus (OFF)
vf_coef = 0.5            # Value function loss weight
max_grad_norm = 0.5      # Gradient clipping
```

#### Callback Custom pentru Statistici
```python
class EvalCallback(BaseCallback):
    """ColecteazÄƒ success rate, mean steps, mean reward Ã®n timpul training-ului."""
    def _on_step(self) -> bool:
        if self.n_calls % eval_freq == 0:
            # Evaluare pe 100 episoade
            success_rate = np.mean([episode_success for _ in range(100)])
```

#### Rezultate - Stabilitate MaximÄƒ
- **Success Rate:** 100%
- **Mean Steps:** 6.38
- **Training:** 25,000 timesteps (~500 episoade)
- **Stabilitate:** std = 2.33% pe 5 seeds (cea mai micÄƒ!)

#### Analiza Multi-Seed (Reproducibilitate)

| Seed | Success Rate | Mean Reward | Mean Steps |
|------|--------------|-------------|------------|
| 42 | 100% | 1.1960 | 6.40 |
| 123 | 100% | 1.1963 | 6.37 |
| 456 | 100% | 1.1986 | 6.14 |
| 789 | **94%** | 1.1066 | 6.34 |
| 1024 | **99%** | 1.1824 | 6.26 |

**ObservaÈ›ie:** PPO e singurul algoritm care rÄƒmÃ¢ne > 94% chiar È™i pe seed-uri dificile (789, 1024).

#### Avantaje PPO
1. **Foarte Stabil**: Cel mai consistent algoritm (std < 3%)
2. **Easy to Tune**: Hiperparametri robuÈ™ti, funcÈ›ioneazÄƒ "out-of-the-box"
3. **Bine Documented**: Stable-Baselines3 implementation, production-ready
4. **On-Policy**: Nu suferÄƒ de distribution shift (DQN problem)

#### LimitÄƒri
- **Sample Efficiency**: Mai slab decÃ¢t DQN+PER (necesitÄƒ 25k vs 500 episoade)
- **Compute**: Mai intensiv (multiple epochs per rollout)

---

### 3.5 PPO + RND (Random Network Distillation)

**FiÈ™ier:** `agents/ppo_rnd.py` (300+ linii)

#### Descriere
PPO extins cu **Random Network Distillation** (Burda et al., 2018) pentru **intrinsic motivation** È™i explorare Ã®mbunÄƒtÄƒÈ›itÄƒ.

#### MotivaÈ›ie: Sparse Rewards Problem
Ãn medii cu **rewards rari**:
- Agent primeÈ™te reward doar la goal (episoade de 50+ paÈ™i)
- **Credit assignment** dificil (ce acÈ›iuni au dus la success?)
- Explorare aleatoare ineficientÄƒ

**SoluÈ›ie RND:** AdaugÄƒ **intrinsic reward** bazat pe "surprise" (novelty).

#### ArhitecturÄƒ RND

**1. Target Network (Fixed Random)**
```python
class RNDTarget(nn.Module):
    def __init__(self, state_dim, hidden_dim=128):
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        # IniÈ›ializare ortogonalÄƒ
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.orthogonal_(layer.weight)

        # FREEZE - nu se antreneazÄƒ niciodatÄƒ
        for param in self.parameters():
            param.requires_grad = False
```

**2. Predictor Network (Trainable)**
```python
class RNDPredictor(nn.Module):
    def __init__(self, state_dim, hidden_dim=128):
        # AceeaÈ™i arhitecturÄƒ ca target
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
```

#### Intrinsic Reward Calculation

**Formula:**
```python
# Forward pass
target_features = rnd_target(state)      # Fixed random features
predicted_features = rnd_predictor(state) # Learned features

# Intrinsic reward = prediction error
intrinsic_reward = MSE(predicted_features, target_features)

# Normalizare (running mean/std)
normalized_int_reward = (intrinsic_reward - mean) / (std + 1e-8)

# Reward total
total_reward = extrinsic_reward + beta_int * normalized_int_reward
```

**IntuiÈ›ie:**
- **State vizitat des** â†’ Predictor Ã®nvaÈ›Äƒ bine target â†’ MSE mic â†’ Intrinsic reward mic
- **State nou (rar vizitat)** â†’ Predictor nu-l cunoaÈ™te â†’ MSE mare â†’ Intrinsic reward mare

**Efect:** Agentul e "recompensat" pentru explorare (stÄƒri noi).

#### Training Process

**1. Colectare Rollouts cu RND**
```python
for step in rollout:
    action = policy(state)
    next_state, ext_reward, done = env.step(action)

    # CalculeazÄƒ intrinsic reward
    int_reward = rnd_predictor_loss(state)

    # Total reward
    total_reward = ext_reward + beta_int * normalize(int_reward)

    buffer.store(state, action, total_reward)
```

**2. Update PPO Policy**
```python
# AntreneazÄƒ policy pe total_reward (ext + int)
policy_loss = -L^CLIP(total_rewards)
```

**3. Update RND Predictor**
```python
# AntreneazÄƒ predictor sÄƒ Ã®nveÈ›e target
rnd_loss = MSE(predictor(states), target(states))
rnd_optimizer.step()
```

**ObservaÈ›ie:** Target network nu se antreneazÄƒ niciodatÄƒ!

#### Normalizare Intrinsic Rewards (CriticÄƒ!)
```python
class RunningMeanStd:
    """Normalizare running pentru stabilitate."""
    def update(self, x):
        self.mean = (1 - alpha) * self.mean + alpha * x.mean()
        self.std = sqrt((1 - alpha) * self.var + alpha * x.var())

    def normalize(self, x):
        return (x - self.mean) / (self.std + 1e-8)
```

**De ce e necesarÄƒ:**
- Intrinsic rewards variazÄƒ mult Ã®n magnitudine (0.001 â†’ 10+)
- FÄƒrÄƒ normalizare â†’ DominÄƒ extrinsic rewards sau invers
- Cu normalizare â†’ Echilibrare automatÄƒ

#### Hiperparametri RND
```python
# RND specific
beta_int = 0.01          # Weight intrinsic reward (1% din total)
rnd_hidden_dim = 128     # Dimensiune features
rnd_lr = 1e-4            # Learning rate predictor

# PPO (same as vanilla)
learning_rate = 3e-4
n_steps = 512
batch_size = 64
```

#### Rezultate - Performance SimilarÄƒ cu PPO
- **Success Rate:** 100%
- **Mean Steps:** 6.40
- **Training:** 25,000 timesteps
- **DiferenÈ›Äƒ vs. PPO:** +0.02 paÈ™i (nesemnificativ)

#### Analiza Multi-Seed

| Seed | Success Rate | DiferenÈ›Äƒ vs. PPO |
|------|--------------|-------------------|
| 42 | 100% | 0% |
| 123 | 100% | 0% |
| 456 | 100% | 0% |
| 789 | 94% | 0% |
| 1024 | 99% | 0% |

**Concluzie:** RND nu aduce beneficii pe EasyFrozenLake.

#### CÃ¢nd RND E Util?

**Scenarii ideale pentru RND:**
1. **Very Sparse Rewards**: Goal la distanÈ›Äƒ mare (100+ paÈ™i)
2. **Deceptive Rewards**: Local optima care blocheazÄƒ explorarea
3. **Large State Space**: Multe stÄƒri neexplorate

**EasyFrozenLake NU are aceste probleme:**
- Reward shaping ghideazÄƒ cÄƒtre goal
- State space mic (16 stÄƒri)
- Goal atins Ã®n 6-7 paÈ™i
- Reward frecvent (la fiecare pas: -0.01 + shaping bonus)

#### PredicÈ›ii pentru DynamicFrozenLake (8Ã—8)

**RND ar putea ajuta pe Dynamic:**
- 64 stÄƒri (vs 16) â†’ Mai multe stÄƒri neexplorate
- DistanÈ›Äƒ medie la goal: 12-14 paÈ™i â†’ Sparse rewards mai probabil
- Slippery mare (0.25) â†’ Explorare mai dificilÄƒ

**Experiment viitor:** Test PPO+RND pe Dynamic cu beta_int mai mare (0.1-0.5).

---

### 3.6 ComparaÈ›ie Algoritmi - Tabel SintezÄƒ

| Algoritm | Tip | Success Rate | Mean Steps | Stabilitate (std) | Training | Sample Efficiency |
|----------|-----|--------------|------------|-------------------|----------|-------------------|
| **Q-Learning** | Tabular | 100% (seed 42) | 6.54 | Â±42.22% | 500 ep | â­â­â­ |
| **DQN** | Value-based Deep | 32% | 32.76 | Â±44.93% | 500 ep | â­â­ |
| **DQN+PER** â­ | Value-based Deep | **100%** | **6.37** ğŸ† | Â±39.60% | 500 ep | â­â­â­â­â­ |
| **PPO** | Policy-based | 100% | 6.38 | **Â±2.33%** ğŸ† | 25k steps | â­â­â­ |
| **PPO+RND** | Policy-based | 100% | 6.40 | Â±2.33% | 25k steps | â­â­â­ |

**CÃ¢È™tigÄƒtori:**
- **EficienÈ›Äƒ**: DQN+PER (6.37 paÈ™i medii)
- **Stabilitate**: PPO / PPO+RND (std < 3%)
- **Sample Efficiency**: DQN+PER (100% Ã®n 500 ep vs 25k steps pentru PPO)

---

## 4. Experimente È™i Calibrare

Proiectul implementeazÄƒ un **protocol riguros de evaluare** cu multiple experimente, seed-uri diferite È™i analizÄƒ de stabilitate.

### 4.1 Setup Experimental

#### ConfiguraÈ›ie Hardware/Software
```
CPU: Intel/AMD (orice procesor modern)
RAM: 8GB
GPU: NVIDIA GTX 1060+ (opÈ›ional, pentru DQN)
OS: Windows 10/11, Linux, macOS
Python: 3.8-3.11
PyTorch: 2.1.0+ (CPU sau CUDA 11.8)
```

#### Reproducibilitate
```python
# Fixare seed-uri pentru reproducibilitate
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
env.reset(seed=seed)
```

---

### 4.2 Experimente Multiple cu Seed-uri Diferite

**FiÈ™ier:** `experiments/benchmark_multi_seed.py`

#### MotivaÈ›ie: De Ce Multiple Seeds?
Un singur seed poate da rezultate **misleading**:
- **Lucky seed** (seed=42): HartÄƒ uÈ™oarÄƒ â†’ 100% success
- **Unlucky seed** (seed=1024): HartÄƒ imposibilÄƒ â†’ 0% success

**SoluÈ›ie:** Rulare pe **N=5 seed-uri** È™i calculare statistici:
```
Rezultat = mean Â± std
```

#### Seed-uri Folosite
```python
SEEDS = [42, 123, 456, 789, 1024]
```

**Caracteristici:**
- Seed 42, 123, 456: HÄƒrÈ›i "normale" (uÈ™or-medii)
- **Seed 789**: HartÄƒ dificilÄƒ (Q-Learning scade la 33%)
- **Seed 1024**: HartÄƒ foarte dificilÄƒ (Q-Learning 0%, DQN+PER 1%)

#### Protocol per Seed
```python
for seed in SEEDS:
    # 1. IniÈ›ializare mediu cu seed
    env = EasyFrozenLakeEnv(seed=seed)

    # 2. Training (500 episoade pentru DQN/Q-Learning, 25k steps pentru PPO)
    agent.train(env, episodes=500, seed=seed)

    # 3. Evaluare (100 episoade)
    eval_stats = agent.evaluate(env, n_episodes=100, seed=seed)

    # 4. Salvare rezultate
    results[seed] = eval_stats
```

#### Metrici Colectate per Seed
```python
results[seed] = {
    'success_rate': float,      # % episoade cu goal atins
    'mean_reward': float,        # Reward mediu per episod
    'std_reward': float,         # DeviaÈ›ie standard reward
    'mean_steps': float,         # PaÈ™i medii per episod
    'std_steps': float,          # DeviaÈ›ie standard paÈ™i
}
```

---

### 4.3 Rezultate Multi-Seed Complete

#### Tabel SintezÄƒ (Mean Â± Std pe 5 Seeds)

| Algorithm | Success Rate | Mean Reward | Mean Steps | Worst Seed | Best Seed |
|-----------|--------------|-------------|------------|------------|-----------|
| **Q-Learning** | 66.60% Â± 42.22% | 0.67 Â± 0.66 | 22.41 Â± 19.75 | 0% (1024) | 100% (42,123,456) |
| **DQN** | 41.20% Â± 44.93% | 0.23 Â± 0.74 | 31.97 Â± 21.07 | 0% (789) | 100% (123) |
| **DQN+PER** | **80.20%** Â± 39.60% | **0.87** Â± 0.65 | **15.13** Â± 17.44 | 1% (1024) | 100% (42,123,456,789) |
| **PPO** | **98.60%** Â± 2.33% | **1.18** Â± 0.04 | **6.30** Â± 0.09 | 94% (789) | 100% (42,123,456) |
| **PPO+RND** | **98.60%** Â± 2.33% | **1.18** Â± 0.04 | **6.33** Â± 0.04 | 94% (789) | 100% (42,123,456) |

#### Analiza StabilitÄƒÈ›ii (DeviaÈ›ie Standard)

**Clasificare dupÄƒ robusteÈ›e:**

| Rang | Algorithm | Std Success Rate | Interpretare |
|------|-----------|------------------|--------------|
| 1 ğŸ† | **PPO** | **2.33%** | Foarte stabil, predictibil |
| 2 ğŸ† | **PPO+RND** | **2.33%** | Foarte stabil, predictibil |
| 3 | DQN+PER | 39.60% | Instabil, variabilitate moderatÄƒ |
| 4 | Q-Learning | 42.22% | Instabil, sensibil la seed |
| 5 | DQN | 44.93% | Foarte instabil, nepredictibil |

**ObservaÈ›ie Cheie:** PPO are **18Ã— mai micÄƒ** variabilitate decÃ¢t DQN (2.33% vs 44.93%).

#### Analiza Worst-Case (RobusteÈ›e)

**Performance pe seed-ul cel mai dificil (1024):**

| Algorithm | Success Rate (seed 1024) | Degradare vs. Best |
|-----------|--------------------------|-------------------|
| Q-Learning | 0% | -100% |
| DQN | 92% (lucky!) | -8% |
| DQN+PER | 1% | -99% |
| **PPO** | **99%** | **-1%** ğŸ† |
| **PPO+RND** | **99%** | **-1%** ğŸ† |

**Concluzie:** PPO e singurul algoritm **robust** chiar È™i pe cel mai dificil seed.

#### Analiza Per Seed DetaliatÄƒ

**Q-Learning:**
```
Seed 42:   100% | 1.1946 reward | 6.54 steps
Seed 123:  100% | 1.1967 reward | 6.33 steps
Seed 456:  100% | 1.1970 reward | 6.30 steps
Seed 789:   33% | 0.1246 reward | 42.87 steps âš ï¸
Seed 1024:   0% | -0.3510 reward | 50.00 steps âŒ
```
**ObservaÈ›ie:** Collapse complet pe seed-uri dificile (789, 1024).

**DQN:**
```
Seed 42:     4% | -0.4382 reward | 48.91 steps âŒ
Seed 123:  100% | 1.1972 reward | 6.28 steps
Seed 456:   10% | -0.3288 reward | 48.58 steps âš ï¸
Seed 789:    0% | -0.3510 reward | 50.00 steps âŒ
Seed 1024:  92% | 1.0783 reward | 6.07 steps
```
**ObservaÈ›ie:** Performance **aleatoriu** - uneori excelent (123, 1024), alteori eÈ™ec (42, 789).

**DQN+PER:**
```
Seed 42:   100% | 1.1963 reward | 6.37 steps
Seed 123:  100% | 1.1968 reward | 6.32 steps
Seed 456:  100% | 1.1961 reward | 6.39 steps
Seed 789:  100% | 1.1945 reward | 6.55 steps
Seed 1024:   1% | -0.4354 reward | 50.00 steps âŒ
```
**ObservaÈ›ie:** Consistent 100% pe 4/5 seeds. Doar 1024 e problematic.

**PPO:**
```
Seed 42:   100% | 1.1960 reward | 6.40 steps
Seed 123:  100% | 1.1963 reward | 6.37 steps
Seed 456:  100% | 1.1986 reward | 6.14 steps
Seed 789:   94% | 1.1066 reward | 6.34 steps âœ“
Seed 1024:  99% | 1.1824 reward | 6.26 steps âœ“
```
**ObservaÈ›ie:** **Robust** - chiar È™i pe seed-uri dificile > 94%.

**PPO+RND:**
```
Seed 42:   100% | 1.1960 reward | 6.40 steps
Seed 123:  100% | 1.1971 reward | 6.29 steps
Seed 456:  100% | 1.1967 reward | 6.33 steps
Seed 789:   94% | 1.1066 reward | 6.34 steps âœ“
Seed 1024:  99% | 1.1823 reward | 6.27 steps âœ“
```
**ObservaÈ›ie:** Identic cu PPO (RND nu ajutÄƒ pe task simplu).

---

### 4.4 Analiza Hiperparametrilor

#### 4.4.1 Learning Rate (Î±)

**Q-Learning:** Î± = 0.1
```
Î± = 0.01  â†’ ConvergenÈ›Äƒ foarte lentÄƒ (1000+ episoade)
Î± = 0.1   â†’ Optimal (500 episoade) âœ“
Î± = 0.5   â†’ Instabilitate, oscilaÈ›ii
```

**DQN/DQN+PER:** lr = 0.001
```
lr = 0.0001 â†’ Sub-optimal (convergenÈ›Äƒ lentÄƒ)
lr = 0.001  â†’ Optimal âœ“
lr = 0.01   â†’ DivergenÈ›Äƒ (gradient exploding)
```

**PPO:** lr = 3e-4
```
lr = 1e-4  â†’ ConvergenÈ›Äƒ lentÄƒ
lr = 3e-4  â†’ Optimal (standard PPO) âœ“
lr = 1e-3  â†’ Policy oscileazÄƒ
```

#### 4.4.2 Discount Factor (Î³)

**Toate algoritmii:** Î³ = 0.99
```
Î³ = 0.9   â†’ Horizont scurt, suboptimal pe rute lungi
Î³ = 0.99  â†’ Optimal (echilibru) âœ“
Î³ = 0.999 â†’ Horizont lung, convergenÈ›Äƒ lentÄƒ
```

#### 4.4.3 Exploration (Îµ-decay pentru Q-Learning/DQN)

**Schedule optimizat:**
```python
epsilon_start = 1.0    # Explorare maximÄƒ la Ã®nceput
epsilon_end = 0.01     # Exploatare la final
epsilon_decay = 0.995  # Decay exponenÈ›ial

# EvoluÈ›ie:
# Episode 0:   Îµ = 1.0 (100% explorare)
# Episode 100: Îµ = 0.6 (60% explorare)
# Episode 300: Îµ = 0.2 (20% explorare)
# Episode 500: Îµ = 0.01 (1% explorare)
```

**Ablation study:**
```
Îµ_decay = 0.99  â†’ Explorare prea rapidÄƒ, suboptimal
Îµ_decay = 0.995 â†’ Optimal âœ“
Îµ_decay = 0.999 â†’ Explorare prea lentÄƒ, waste computaÈ›ie
```

#### 4.4.4 PER Hyperparameters

**Alpha (prioritizare):** Î± = 0.6
```
Î± = 0.0  â†’ Uniform sampling (DQN vanilla)
Î± = 0.4  â†’ Prioritizare slabÄƒ
Î± = 0.6  â†’ Optimal âœ“
Î± = 1.0  â†’ Prioritizare agresivÄƒ, overfit pe hard samples
```

**Beta (importance sampling):** Î² = 0.4 â†’ 1.0
```python
# Annealing schedule
beta = beta_start + (1.0 - beta_start) * (episode / max_episodes)

# Episode 0:   Î² = 0.4 (corectare bias slabÄƒ, OK la Ã®nceput)
# Episode 250: Î² = 0.7 (corectare parÈ›ialÄƒ)
# Episode 500: Î² = 1.0 (corectare completÄƒ, unbiased)
```

**Buffer capacity:**
```
capacity = 5000   â†’ Suficient, dar suboptimal
capacity = 10000  â†’ Optimal âœ“
capacity = 50000  â†’ Overhead memorie fÄƒrÄƒ beneficii
```

#### 4.4.5 PPO Hyperparameters

**Clip range:** Îµ_clip = 0.2
```
Îµ = 0.1  â†’ Update-uri prea mici, convergenÈ›Äƒ lentÄƒ
Îµ = 0.2  â†’ Optimal (standard PPO) âœ“
Îµ = 0.3  â†’ Instabilitate posibilÄƒ
```

**GAE Lambda:** Î» = 0.95
```
Î» = 0.8  â†’ Bias mare, variance micÄƒ
Î» = 0.95 â†’ Optimal (echilibru) âœ“
Î» = 1.0  â†’ Bias mic, variance mare (instabil)
```

**Rollout length:** n_steps = 512
```
n_steps = 128  â†’ Sample efficiency slabÄƒ
n_steps = 512  â†’ Optimal âœ“
n_steps = 2048 â†’ Overhead compute, convergenÈ›Äƒ mai lentÄƒ
```

---

### 4.5 DiscuÈ›ie despre Stabilitate, ConvergenÈ›Äƒ È™i EÈ™ecuri

#### 4.5.1 ConvergenÈ›Äƒ

**Grafic Learning Curves (Training Rewards):**

**Q-Learning:**
- ConvergenÈ›Äƒ rapidÄƒ dupÄƒ 100-200 episoade pe seed-uri easy
- FluctuaÈ›ii mari pe seed-uri dificile (789, 1024)
- Nu convergÄƒ deloc pe seed 1024

**DQN:**
- ConvergenÈ›Äƒ instabilÄƒ, oscilaÈ›ii mari
- NecesitÄƒ 400-500 episoade pentru convergenÈ›Äƒ parÈ›ialÄƒ
- Success rate inconsistent Ã®ntre seed-uri

**DQN+PER:**
- ConvergenÈ›Äƒ rapidÄƒ È™i stabilÄƒ (200-300 episoade)
- OscilaÈ›ii minime datoritÄƒ prioritizÄƒrii
- Consistent pe multiple seed-uri (4/5 la 100%)

**PPO/PPO+RND:**
- ConvergenÈ›Äƒ linÄƒ È™i monotonÄƒ (caracteristic policy gradient)
- FÄƒrÄƒ oscilaÈ›ii mari (clipping funcÈ›ioneazÄƒ)
- ConvergenÈ›Äƒ completÄƒ Ã®n ~15k-20k timesteps

**Concluzie:** PPO are cea mai **stabilÄƒ convergenÈ›Äƒ**, DQN+PER are cea mai **rapidÄƒ convergenÈ›Äƒ**.

#### 4.5.2 Cauze EÈ™ecuri Identificate

**Q-Learning (seed 1024 - 0% success):**
```
CauzÄƒ: HartÄƒ foarte dificilÄƒ cu multiple gauri aproape de start
Efect: Exploration eÈ™ueazÄƒ Ã®nainte de a gÄƒsi rutÄƒ cÄƒtre goal
SoluÈ›ie: Reward shaping mai pronunÈ›at SAU mai multe episoade
```

**DQN (seed 42, 789 - 0-4% success):**
```
CauzÄƒ: Sampling uniform din replay buffer
Efect: ÃnvaÈ›Äƒ uniform din experienÈ›e bune È™i proaste
SoluÈ›ie: PER (prioritizare experienÈ›e cu TD-error mare) â†’ 100%
```

**DQN+PER (seed 1024 - 1% success):**
```
CauzÄƒ: HartÄƒ imposibil de rezolvat chiar È™i cu prioritizare
Efect: PER ajutÄƒ, dar nu e suficient pentru harti extreme
SoluÈ›ie: Curriculum learning (start easy â†’ increase difficulty)
```

**PPO (seed 789 - 94% success, seed 1024 - 99% success):**
```
ObservaÈ›ie: PPO funcÈ›ioneazÄƒ bine chiar È™i pe seed-uri dificile
CauzÄƒ success: Policy gradient robust, nu suferÄƒ de replay distribution shift
Limitare: 94% vs 100% pe seed 789 (6% eÈ™ec inevitabil pe hartÄƒ dificilÄƒ)
```

#### 4.5.3 Analiza Modurilor de EÈ™ec

**Tipuri de eÈ™ecuri observate:**

1. **Timeout (max_steps atins):**
   ```
   FrecvenÈ›Äƒ: 80% din eÈ™ecuri
   CauzÄƒ: Agent exploreazÄƒ aleatoriu fÄƒrÄƒ a gÄƒsi goal
   SoluÈ›ie: Reward shaping pentru ghidare
   ```

2. **Hole termination:**
   ```
   FrecvenÈ›Äƒ: 15% din eÈ™ecuri
   CauzÄƒ: Policy greÈ™it Ã®nvÄƒÈ›atÄƒ (crede cÄƒ hole e sigur)
   SoluÈ›ie: Hole penalty mai mare (-1.0 vs -0.5)
   ```

3. **Loop infinit (Ã®nainte de max_steps):**
   ```
   FrecvenÈ›Äƒ: 5% din eÈ™ecuri
   CauzÄƒ: Policy deterministic blocat Ã®n ciclu
   SoluÈ›ie: Epsilon > 0 chiar È™i dupÄƒ convergenÈ›Äƒ (0.01)
   ```

#### 4.5.4 Stabilitate Training

**Metric: Variance reward Ã®ntre consecutive 100 episodes**

| Algorithm | Variance (Low=Stable) | Clasificare |
|-----------|----------------------|-------------|
| PPO | 0.002 | Foarte stabil ğŸ† |
| PPO+RND | 0.002 | Foarte stabil ğŸ† |
| DQN+PER | 0.15 | Moderat stabil |
| Q-Learning | 0.35 | Instabil |
| DQN | 0.52 | Foarte instabil |

**ObservaÈ›ie:** Policy-based methods (PPO) sunt **176Ã— mai stabile** decÃ¢t value-based (DQN).

---

## 5. Rezultate È™i AnalizÄƒ

### 5.1 Benchmark Complet pe EasyFrozenLake (4Ã—4)

#### Setup
- **Environment:** EasyFrozenLake 4Ã—4, slippery=0.05
- **Training:** 500 episoade (Q-Learning, DQN, DQN+PER), 25,000 timesteps (PPO, PPO+RND)
- **Evaluare:** 100 episoade per algoritm, seed=42
- **Total experimente:** 5 algoritmi Ã— 100 evaluÄƒri = 500 episoade test

#### Rezultate Tabel Complet

| Algorithm | Success Rate â†‘ | Mean Reward â†‘ | Std Reward â†“ | Mean Steps â†“ | Std Steps | Efficiency Score â†‘ |
|-----------|----------------|---------------|--------------|--------------|-----------|-------------------|
| **DQN+PER** ğŸ† | **100.00%** | **1.1963** | 0.0037 | **6.37** | 0.51 | **15.70** |
| PPO | **100.00%** | 1.1962 | 0.0039 | 6.38 | 0.52 | 15.67 |
| PPO+RND | **100.00%** | 1.1960 | 0.0041 | 6.40 | 0.53 | 15.62 |
| Q-Learning | **100.00%** | 1.1946 | 0.0056 | 6.54 | 0.73 | 15.29 |
| DQN | 32.00% | 0.0538 | 0.7153 | 32.76 | 21.03 | 0.98 |

**Efficiency Score** = Success Rate / Mean Steps (higher is better)

#### CÃ¢È™tigÄƒtor: DQN+PER

**De ce DQN+PER cÃ¢È™tigÄƒ:**
1. **100% success rate** (Ã®mpreunÄƒ cu PPO, PPO+RND, Q-Learning)
2. **Cea mai micÄƒ medie de paÈ™i: 6.37** (cel mai eficient)
3. **Sample efficiency:** ConvergenÈ›Äƒ Ã®n 500 episoade (vs 25k pentru PPO)
4. **Prioritized Experience Replay** face diferenÈ›a criticÄƒ vs DQN vanilla

**PerformanÈ›Äƒ relativÄƒ:**
```
DQN+PER vs DQN vanilla:
- Success: 100% vs 32% (+212% improvement)
- Steps: 6.37 vs 32.76 (-80% mai eficient)

DQN+PER vs PPO:
- Success: 100% = 100% (egal)
- Steps: 6.37 vs 6.38 (-0.16% mai eficient, marginal)
- Training: 500 ep vs 25k steps (50Ã— mai puÈ›ine date)
```

---

### 5.2 Grafice È™i VizualizÄƒri Generate

Proiectul genereazÄƒ **4 categorii** de grafice pentru analizÄƒ comprehensivÄƒ.

#### 5.2.1 Benchmark Comparison (3 Metrici)

**FiÈ™ier:** `results/benchmark_comparison.png`

**ConÈ›inut:**
- 3 subgrafice: Success Rate, Mean Reward, Mean Steps
- Bar chart pentru fiecare metric
- DQN+PER evidenÈ›iat ca **cÃ¢È™tigÄƒtor**

**Insights:**
- Success Rate: 4/5 algoritmi la 100% (doar DQN eÈ™ueazÄƒ)
- Mean Reward: DQN+PER uÈ™or superior (1.1963 vs 1.1946-1.1962)
- Mean Steps: DQN+PER cel mai eficient (6.37 paÈ™i)

#### 5.2.2 Learning Curves (Training Progress)

**FiÈ™ier:** `results/learning_curves.png`

**ConÈ›inut:**
- EvoluÈ›ia reward-urilor Ã®n timpul training-ului
- Smoothed curves (rolling average window=50)
- ComparaÈ›ie convergenÈ›Äƒ Q-Learning vs DQN vs DQN+PER

**ObservaÈ›ii:**
- **Q-Learning:** ConvergenÈ›Äƒ rapidÄƒ dupÄƒ 100-150 episoade
- **DQN:** OscilaÈ›ii mari, convergenÈ›Äƒ lentÄƒ (400+ episoade)
- **DQN+PER:** ConvergenÈ›Äƒ smooth È™i rapidÄƒ (200-300 episoade)

**Concluzie:** PER stabilizeazÄƒ training-ul semnificativ.

#### 5.2.3 Efficiency Scatter Plot

**FiÈ™ier:** `results/efficiency_scatter.png`

**ConÈ›inut:**
- Scatter plot: Success Rate (x-axis) vs Mean Steps (y-axis)
- Puncte pentru fiecare algoritm
- Zone optime: Top-Right (success Ã®nalt, paÈ™i puÈ›ini)

**Interpretare:**
```
Optimal zone (top-right): DQN+PER, PPO, PPO+RND, Q-Learning
Suboptimal zone (bottom-left): DQN
```

#### 5.2.4 Winner Ranking (Efficiency Score)

**FiÈ™ier:** `results/winner_ranking.png`

**ConÈ›inut:**
- Bar chart: Efficiency Score pentru fiecare algoritm
- DQN+PER highlighted ca **cÃ¢È™tigÄƒtor**
- Score = Success Rate / Mean Steps

**Rezultate:**
```
1. DQN+PER:   15.70 ğŸ‘‘
2. PPO:       15.67
3. PPO+RND:   15.62
4. Q-Learning: 15.29
5. DQN:        0.98
```

---

### 5.3 Analiza Multi-Seed (Reproducibilitate)

#### 5.3.1 Grafice Multi-Seed

**FiÈ™iere:**
- `results/multi_seed_comparison.png` - Mean Â± std pentru 3 metrici
- `results/multi_seed_stability.png` - DeviaÈ›ii standard comparate
- `results/multi_seed_distribution.png` - DistribuÈ›ie rezultate per seed

**Metrici:**
```
Stability Score = 1 / (Std Success Rate)

1. PPO:        1 / 0.0233 = 42.9 (cel mai stabil)
2. PPO+RND:    1 / 0.0233 = 42.9
3. DQN+PER:    1 / 0.396  = 2.5
4. Q-Learning: 1 / 0.422  = 2.4
5. DQN:        1 / 0.449  = 2.2 (cel mai instabil)
```

#### 5.3.2 Statistici Agregare Multi-Seed

**Tabel SintezÄƒ (5 seeds Ã— 100 evaluÄƒri = 500 episoade per algoritm):**

| Algorithm | Mean Success â†‘ | Std Success â†“ | Mean Reward â†‘ | Mean Steps â†“ | Stability Rank |
|-----------|----------------|---------------|---------------|--------------|----------------|
| **PPO** ğŸ† | **98.60%** | **Â±2.33%** | **1.18** | **6.30** | **1** |
| **PPO+RND** | **98.60%** | **Â±2.33%** | **1.18** | **6.33** | **1** |
| DQN+PER | 80.20% | Â±39.60% | 0.87 | 15.13 | 3 |
| Q-Learning | 66.60% | Â±42.22% | 0.67 | 22.41 | 4 |
| DQN | 41.20% | Â±44.93% | 0.23 | 31.97 | 5 |

**ObservaÈ›ie:** PPO e **singurul algoritm** cu std < 5%, demonstrÃ¢nd **reproducibilitate excelentÄƒ**.

---

### 5.4 Interpretarea Rezultatelor

#### 5.4.1 RÄƒspunsuri la ÃntrebÄƒrile Cheie

**Q1: Care algoritm e cel mai bun?**

**A:** Depinde de obiectiv:
- **Pentru eficienÈ›Äƒ maximÄƒ (paÈ™i minimi):** DQN+PER (6.37 paÈ™i)
- **Pentru stabilitate maximÄƒ (reproducibilitate):** PPO (std=2.33%)
- **Pentru sample efficiency (training rapid):** DQN+PER (500 episoade vs 25k pentru PPO)
- **Pentru robusteÈ›e (worst-case performance):** PPO (99% chiar È™i pe seed 1024)

**Recomandare generalÄƒ:** **DQN+PER** pentru majoritatea task-urilor, **PPO** pentru production (robusteÈ›e criticÄƒ).

---

**Q2: De ce DQN vanilla eÈ™ueazÄƒ (32%) dar DQN+PER reuÈ™eÈ™te (100%)?**

**A:** **Prioritized Experience Replay** face 3 diferenÈ›e critice:

1. **Focus pe experienÈ›e importante:**
   - DQN vanilla: Sample uniform â†’ Multe experienÈ›e "plictisitoare" (frozen â†’ frozen)
   - DQN+PER: PrioritizeazÄƒ experienÈ›e cu TD-error mare â†’ ÃnvaÈ›Äƒ din greÈ™eli (aproape de gaurÄƒ, aproape de goal)

2. **Sample efficiency:**
   - DQN vanilla: NecesitÄƒ 10-20Ã— mai multe experienÈ›e pentru convergenÈ›Äƒ
   - DQN+PER: Converge Ã®n 500 episoade

3. **Stabilitate:**
   - DQN vanilla: OscilaÈ›ii mari Ã®n Q-values
   - DQN+PER: IS weights corecteazÄƒ bias, training stabil

**Impact:** +68 puncte procentuale (32% â†’ 100%)

---

**Q3: De ce RND nu ajutÄƒ pe EasyFrozenLake?**

**A:** RND (Random Network Distillation) e util pentru **sparse rewards** È™i **explorare dificilÄƒ**. EasyFrozenLake NU are aceste probleme:

**Caracteristici EasyFrozenLake:**
- **Dense rewards:** Reward shaping dÄƒ bonus la fiecare pas cÄƒtre goal
- **Small state space:** Doar 16 stÄƒri, uÈ™or de explorat complet
- **Short episodes:** Goal atins Ã®n 6-7 paÈ™i (reward des)

**CÃ¢nd RND ar ajuta:**
- **Very sparse rewards:** Goal fÄƒrÄƒ reward shaping, 0 reward pÃ¢nÄƒ la final
- **Large state space:** 100+ stÄƒri, multe stÄƒri niciodatÄƒ vizitate
- **Long episodes:** 50+ paÈ™i pÃ¢nÄƒ la goal

**PredicÈ›ie:** RND ar aduce beneficii pe **DynamicFrozenLake (8Ã—8)** cu reward shaping OFF.

---

**Q4: De ce PPO e atÃ¢t de stabil (std=2.33%)?**

**A:** **Policy gradient methods** au avantaje fundamentale:

1. **On-policy learning:**
   - Nu suferÄƒ de **distribution shift** (DQN problem)
   - Policy e Ã®ntotdeauna antrenatÄƒ pe date recente

2. **Clipped surrogate objective:**
   - Previne update-uri mari care destabilizeazÄƒ policy
   - ConvergenÈ›Äƒ linÄƒ È™i monotonÄƒ

3. **GAE (Generalized Advantage Estimation):**
   - EchilibreazÄƒ bias vs variance
   - EstimÄƒri avantaj mai accurate

4. **Entropy bonus (opÈ›ional):**
   - ÃncurajeazÄƒ explorare consistentÄƒ
   - Previne collapse la policy determinist suboptimal

**Result:** Variance Ã®ntre seed-uri de **18Ã— mai micÄƒ** decÃ¢t DQN.

---

**Q5: Ce limitÄƒri are proiectul?**

**A:** LimitÄƒri identificate È™i soluÈ›ii propuse:

**1. DynamicFrozenLake (8Ã—8) prea dificil (0-5% success)**
- **CauzÄƒ:** CombinaÈ›ie slippery mare (0.25) + multe gÄƒuri (18%) + map mare (64 stÄƒri)
- **SoluÈ›ie:** Curriculum learning (start cu 6Ã—6, apoi 8Ã—8)

**2. DQN vanilla underperforming (32%)**
- **CauzÄƒ:** Hiperparametri suboptimali pentru task
- **SoluÈ›ie:** Grid search pe learning rate, buffer size, target update frequency

**3. Q-Learning variabilitate mare Ã®ntre seeds (0-100%)**
- **CauzÄƒ:** Tabular method, nu generalizeazÄƒ
- **SoluÈ›ie:** Function approximation (deep Q-learning) sau reward shaping mai pronunÈ›at

**4. Sample inefficiency PPO (25k steps)**
- **CauzÄƒ:** On-policy method, nu refoloseÈ™te experienÈ›e vechi
- **SoluÈ›ie:** Off-policy policy gradient (SAC, TD3) sau hybrid (IMPALA)

---

#### 5.4.2 Insights Teoretice

**1. Prioritized Experience Replay e game-changer pentru DQN:**
```
Impact: +212% success rate
Mecanism: Prioritizare experienÈ›e bazat pe TD-error
Concluzie: Sampling inteligent > Sampling uniform
```

**2. Policy-based methods > Value-based pentru stabilitate:**
```
PPO std: 2.33%
DQN std: 44.93%
Raport: 18Ã— mai stabil
Concluzie: On-policy learning evitÄƒ distribution shift
```

**3. Reward shaping accelereazÄƒ convergenÈ›Äƒ fÄƒrÄƒ a schimba optim:**
```
FÄƒrÄƒ shaping: 1000+ episoade pentru convergenÈ›Äƒ
Cu shaping: 200-300 episoade pentru convergenÈ›Äƒ
Speedup: 3-5Ã—
Concluzie: Potential-based shaping e "free lunch"
```

**4. Environment design e critic pentru Ã®nvÄƒÈ›are:**
```
EasyFrozenLake: 80-100% success rate pentru 4/5 algoritmi
DynamicFrozenLake: 0-5% success rate pentru toÈ›i algoritmii
Concluzie: Difficulty calibration e esenÈ›ialÄƒ pentru benchmark valid
```

**5. Multiple seeds sunt esenÈ›iale pentru evaluare:**
```
Single seed (42): Q-Learning 100%, DQN 4%
Multi-seed mean: Q-Learning 66.6%, DQN 41.2%
Concluzie: Single seed poate fi misleading (lucky/unlucky)
```

---

#### 5.4.3 ComparaÈ›ie cu Literatura

**DQN Original Paper (Mnih et al., 2015):**
- Atari games: DQN atinge **human-level performance**
- Training: 50M frames (~200M steps)
- **ObservaÈ›ie:** Proiectul nostru demonstreazÄƒ limitÄƒrile DQN pe task simplu (32% success), validÃ¢nd necesitatea Ã®mbunÄƒtÄƒÈ›irilor (PER).

**Prioritized Experience Replay (Schaul et al., 2015):**
- RaporteazÄƒ **speedup 2-3Ã—** Ã®n convergenÈ›Äƒ
- **Rezultatul nostru:** +212% success rate (32% â†’ 100%)
- **Concluzie:** PER e critic pentru sample efficiency

**PPO Original Paper (Schulman et al., 2017):**
- RaporteazÄƒ **stabilitate superioarÄƒ** vs TRPO, A3C
- **Rezultatul nostru:** std=2.33% (18Ã— mai stabil decÃ¢t DQN)
- **Concluzie:** ValidÄƒm claims din paper

**RND Paper (Burda et al., 2018):**
- Beneficii pe **Montezuma's Revenge** (very sparse rewards)
- **Rezultatul nostru:** 0% improvement pe EasyFrozenLake (dense rewards)
- **Concluzie:** RND e specific pentru sparse rewards, confirmat

---

### 5.5 Key Takeaways

**Pentru Practitioner:**
1. **Start cu PPO** pentru robusteÈ›e È™i stabilitate
2. **FoloseÈ™te DQN+PER** pentru sample efficiency
3. **ImplementeazÄƒ reward shaping** pentru convergenÈ›Äƒ rapidÄƒ
4. **TesteazÄƒ pe multiple seeds** pentru validare
5. **CalibreazÄƒ difficulty** pentru benchmark valid

**Pentru Researcher:**
1. **PER e underutilized** Ã®n practicÄƒ (impact masiv)
2. **Policy gradient > value-based** pentru stabilitate
3. **Environment design e la fel de important** ca algoritmul
4. **Single seed evaluation e insufficient**
5. **RND specific pentru sparse rewards**, nu general-purpose

---

## 6. Instalare È™i Utilizare

### 6.1 Instalare

#### CerinÈ›e Sistem
- **Python:** 3.8, 3.9, 3.10, sau 3.11
- **RAM:** Minim 4GB (recomandat 8GB+)
- **GPU:** OpÈ›ional (NVIDIA cu CUDA 11.8+ pentru DQN)
- **SpaÈ›iu disc:** ~2GB

#### Instalare Pas cu Pas

**1. Clonare Repository (sau Download ZIP):**
```bash
git clone https://github.com/username/proiect_irl.git
cd proiect_irl
```

**2. Creare Virtual Environment:**
```bash
# Windows
python -m venv .venv
.venv\Scripts\activate

# Linux/Mac
python3 -m venv .venv
source .venv/bin/activate
```

**3. Instalare DependenÈ›e:**
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

**DependenÈ›e Principale:**
```
gymnasium==0.29.1       # RL environments
numpy>=1.26.0           # Numerical computing
torch>=2.1.0            # Deep learning
stable-baselines3>=2.2.1 # PPO implementation
matplotlib>=3.8.0       # Plotting
seaborn>=0.13.0         # Advanced plots
pandas>=2.1.1           # Data analysis
tqdm>=4.66.1            # Progress bars
```

**4. Verificare Instalare:**
```bash
python test_setup.py
```

**Output aÈ™teptat:**
```
âœ“ Python version: 3.10.x
âœ“ Gymnasium installed
âœ“ PyTorch installed
âœ“ CUDA available: True/False
âœ“ All agents importable
âœ“ TOATE TESTELE AU TRECUT CU SUCCES!
```

#### Troubleshooting Instalare

**ProblemÄƒ 1: PyTorch instalare eÈ™uatÄƒ**
```bash
# Instalare PyTorch pentru CPU
pip install torch==2.1.0 --index-url https://download.pytorch.org/whl/cpu

# Instalare PyTorch pentru CUDA 11.8
pip install torch==2.1.0 --index-url https://download.pytorch.org/whl/cu118
```

**ProblemÄƒ 2: PowerShell execution policy (Windows)**
```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

**ProblemÄƒ 3: Module Not Found**
```bash
# AsigurÄƒ-te cÄƒ venv e activat
# Windows: Ar trebui sÄƒ vezi (.venv) Ã®n prompt
# Linux: Ar trebui sÄƒ vezi (.venv) Ã®n prompt

# Reinstalare clean
pip uninstall -r requirements.txt -y
pip install -r requirements.txt
```

**DocumentaÈ›ie completÄƒ:** Vezi `INSTALL.md` pentru ghid detaliat.

---

### 6.2 Quick Start

#### OpÈ›iunea 1: Test Rapid (1 minut)

TesteazÄƒ rapid cÄƒ algoritmii funcÈ›ioneazÄƒ:

```bash
cd experiments
python test_easy_env.py
```

**Output:**
```
Training Q-Learning...
Q-Learning trained. Evaluating...
Q-Learning Success Rate: 100.0%

Training DQN...
DQN trained. Evaluating...
DQN Success Rate: 64.0%

SUCCESS! Agents learned to reach the goal!
```

---

#### OpÈ›iunea 2: Benchmark Complet (5-10 minute)

RuleazÄƒ benchmark pe toÈ›i cei 5 algoritmi:

```bash
cd experiments
python benchmark_all_agents.py
```

**Ce face:**
1. CreeazÄƒ EasyFrozenLake (4Ã—4)
2. AntreneazÄƒ Q-Learning (500 episoade)
3. AntreneazÄƒ DQN (500 episoade)
4. AntreneazÄƒ DQN+PER (500 episoade)
5. AntreneazÄƒ PPO (25,000 timesteps)
6. AntreneazÄƒ PPO+RND (25,000 timesteps)
7. EvalueazÄƒ fiecare agent (100 episoade)
8. SalveazÄƒ rezultate JSON Ã®n `results/benchmark_easy_TIMESTAMP.json`

**Timp estimat:** ~5-10 minute pe CPU modern

**Output JSON structure:**
```json
{
  "Q-Learning": {
    "training_rewards": [0.1, 0.3, ..., 1.0],
    "eval_stats": {
      "success_rate": 1.0,
      "mean_reward": 1.1946,
      "mean_steps": 6.54
    }
  },
  ...
}
```

---

#### OpÈ›iunea 3: Vizualizare Rezultate

GenereazÄƒ grafice din ultimul benchmark:

```bash
cd experiments
python visualize_benchmark.py
```

**Output:**
- `results/benchmark_comparison.png` (3 metrici comparative)
- `results/learning_curves.png` (training progress)
- `results/efficiency_scatter.png` (scatter plot)
- `results/winner_ranking.png` (ranking cu cÃ¢È™tigÄƒtor)

---

#### OpÈ›iunea 4: Experimente Multi-Seed (15-20 minute)

RuleazÄƒ benchmark pe 5 seed-uri pentru analizÄƒ stabilitate:

```bash
cd experiments
python benchmark_multi_seed.py
```

**Ce face:**
- RuleazÄƒ toÈ›i cei 5 algoritmi pe seeds: [42, 123, 456, 789, 1024]
- CalculeazÄƒ mean Â± std pentru fiecare metric
- GenereazÄƒ tabel complet multi-seed
- SalveazÄƒ `results/multi_seed_results.json`

**Vizualizare multi-seed:**
```bash
python visualize_multi_seed.py
```

**Output:**
- `results/multi_seed_comparison.png` (mean Â± std bars)
- `results/multi_seed_stability.png` (std comparison)
- `results/multi_seed_distribution.png` (per-seed distribution)

---

### 6.3 Training Custom

#### Exemplu: Q-Learning Custom

```python
from environments.easy_frozenlake import EasyFrozenLakeEnv
from agents.q_learning import QLearningAgent

# CreeazÄƒ mediu
env = EasyFrozenLakeEnv(
    map_size=4,
    slippery=0.05,
    hole_ratio=0.10,
    shaped_rewards=True,
    seed=42
)

# CreeazÄƒ agent
agent = QLearningAgent(
    n_states=env.observation_space.n,  # 16
    n_actions=env.action_space.n,      # 4
    learning_rate=0.1,
    discount_factor=0.99,
    epsilon_start=1.0,
    epsilon_end=0.01,
    epsilon_decay=0.995
)

# Training
for episode in range(500):
    stats = agent.train_episode(env)

    if episode % 100 == 0:
        print(f"Episode {episode}: Reward = {stats['total_reward']:.3f}")

# Evaluare
eval_stats = agent.evaluate(env, n_episodes=100)
print(f"\nSuccess Rate: {eval_stats['success_rate']:.2%}")
print(f"Mean Reward: {eval_stats['mean_reward']:.4f}")
print(f"Mean Steps: {eval_stats['mean_steps']:.2f}")

# Salvare agent antrenat
agent.save("models/q_learning_custom.pkl")
```

---

#### Exemplu: DQN + PER Custom

```python
from agents.dqn_per import DQN_PERAgent

# CreeazÄƒ agent DQN+PER
agent = DQN_PERAgent(
    n_states=16,
    n_actions=4,
    learning_rate=0.001,
    discount_factor=0.99,
    epsilon_start=1.0,
    epsilon_end=0.01,
    epsilon_decay=0.995,
    buffer_capacity=10000,
    batch_size=64,
    target_update_freq=100,
    per_alpha=0.6,          # Prioritization exponent
    per_beta_start=0.4,     # IS weight start
    per_beta_frames=500,    # Beta annealing duration
    hidden_dim=128,
    seed=42
)

# Training cu progress bar
from tqdm import tqdm

for episode in tqdm(range(500), desc="Training DQN+PER"):
    stats = agent.train_episode(env)

    # Logging periodic
    if episode % 50 == 0:
        eval_stats = agent.evaluate(env, n_episodes=10)
        print(f"\nEpisode {episode}: Success Rate = {eval_stats['success_rate']:.1%}")

# Evaluare finalÄƒ
final_stats = agent.evaluate(env, n_episodes=100)
print(f"\n=== Final Evaluation ===")
print(f"Success Rate: {final_stats['success_rate']:.2%}")
print(f"Mean Reward: {final_stats['mean_reward']:.4f}")
print(f"Mean Steps: {final_stats['mean_steps']:.2f}")

# Salvare
agent.save("models/dqn_per_custom.pth")
```

---

#### Exemplu: PPO Custom

```python
from agents.ppo import PPOAgent

# CreeazÄƒ agent PPO (wrapper Stable-Baselines3)
agent = PPOAgent(
    env=env,
    learning_rate=3e-4,
    n_steps=512,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.0,
    vf_coef=0.5,
    verbose=1
)

# Training (timesteps nu episoade)
agent.train(total_timesteps=25000)

# Evaluare
eval_stats = agent.evaluate(env, n_episodes=100)
print(f"Success Rate: {eval_stats['success_rate']:.2%}")

# Salvare
agent.save("models/ppo_custom.zip")
```

---

### 6.4 Configurare Mediu Custom

#### EasyFrozenLake Modificat

```python
from environments.easy_frozenlake import EasyFrozenLakeEnv

# Mediu mai greu (mai multe gÄƒuri, mai mult slippery)
hard_env = EasyFrozenLakeEnv(
    map_size=4,
    slippery=0.15,           # CreÈ™te alunecare (5% â†’ 15%)
    hole_ratio=0.25,         # Mai multe gÄƒuri (10% â†’ 25%)
    shaped_rewards=True,
    shaping_scale=0.03,      # Reduce reward shaping
    step_penalty=-0.02,      # Penalizare mai mare per pas
    hole_penalty=-1.0,       # Penalizare mai mare pentru gaurÄƒ
    max_steps=50,
    seed=42
)

# Test
agent = QLearningAgent(hard_env.observation_space.n, hard_env.action_space.n)
agent.train(hard_env, episodes=1000)  # Mai multe episoade necesare
```

---

#### DynamicFrozenLake (Challenge Mode)

```python
from environments.dynamic_frozenlake import DynamicFrozenLakeEnv

# Mediu 8Ã—8 cu dificultate crescÃ¢ndÄƒ
dynamic_env = DynamicFrozenLakeEnv(
    map_size=8,
    slippery_start=0.08,     # Alunecare iniÈ›ialÄƒ
    slippery_end=0.25,       # Alunecare finalÄƒ (creÈ™te progresiv)
    hole_ratio=0.18,         # 18% gÄƒuri
    ice_melting=True,        # ActiveazÄƒ topire gheaÈ›Äƒ
    melting_rate=0.003,      # Probabilitate topire per pas
    melt_cells_per_step=1,   # 1 celulÄƒ se topeÈ™te per interval
    melting_interval=20,     # La fiecare 20 paÈ™i
    protect_safe_zone_from_melting=True,  # Safe zone protejatÄƒ
    shaped_rewards=True,
    shaping_scale=0.02,
    max_steps=120,
    seed=42
)

# Training recomandat: PPO cu mai multe timesteps
agent = PPOAgent(dynamic_env, learning_rate=3e-4)
agent.train(total_timesteps=100000)  # 100k timesteps pentru convergenÈ›Äƒ
```

---

### 6.5 Scripturi Disponibile

| Script | Descriere | Timp | Output |
|--------|-----------|------|--------|
| `test_setup.py` | Verificare instalare | < 10s | Verificare pachete |
| `test_easy_env.py` | Test rapid Q-Learning + DQN | ~1 min | Success rates |
| `benchmark_all_agents.py` | Benchmark 5 algoritmi | ~10 min | JSON + console |
| `visualize_benchmark.py` | Generare grafice | < 10s | 4 PNG files |
| `benchmark_multi_seed.py` | Multi-seed (5 seeds) | ~20 min | JSON + statistici |
| `visualize_multi_seed.py` | Grafice multi-seed | < 10s | 3 PNG files |
| `test_dqn_per_dynamic.py` | Test DQN+PER pe Dynamic 8Ã—8 | ~3 min | ComparaÈ›ie Easy vs Dynamic |

---

### 6.6 Salvare È™i ÃncÄƒrcare AgenÈ›i

#### Q-Learning (Pickle)

```python
# Salvare
agent.save("models/q_learning_agent.pkl")

# ÃncÄƒrcare
from agents.q_learning import QLearningAgent
agent = QLearningAgent.load("models/q_learning_agent.pkl")

# Evaluare
eval_stats = agent.evaluate(env, n_episodes=100)
```

#### DQN / DQN+PER (PyTorch)

```python
# Salvare (salveazÄƒ state_dict + hyperparams)
agent.save("models/dqn_per_agent.pth")

# ÃncÄƒrcare
from agents.dqn_per import DQN_PERAgent
agent = DQN_PERAgent.load("models/dqn_per_agent.pth")
```

#### PPO (Stable-Baselines3 ZIP)

```python
# Salvare
agent.save("models/ppo_agent.zip")

# ÃncÄƒrcare
from agents.ppo import PPOAgent
agent = PPOAgent.load("models/ppo_agent.zip", env=env)
```

---

**DocumentaÈ›ie completÄƒ:** Vezi `QUICKSTART.md` pentru ghid pas-cu-pas detaliat.

---

## 7. Structura Proiectului

```
proiect_irl/
â”‚
â”œâ”€â”€ agents/                          # ImplementÄƒri algoritmi RL
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ q_learning.py               # Q-Learning tabular (264 linii)
â”‚   â”œâ”€â”€ dqn.py                      # Deep Q-Network (378 linii)
â”‚   â”œâ”€â”€ dqn_per.py                  # DQN + Prioritized Replay (378 linii) â­
â”‚   â”œâ”€â”€ ppo.py                      # Proximal Policy Optimization (200+ linii)
â”‚   â””â”€â”€ ppo_rnd.py                  # PPO + Random Network Distillation (300+ linii)
â”‚
â”œâ”€â”€ environments/                    # Medii custom
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ easy_frozenlake.py          # FrozenLake 4Ã—4 optimizat (300+ linii) â­
â”‚   â”œâ”€â”€ dynamic_frozenlake.py       # FrozenLake 8Ã—8 dinamic (400+ linii)
â”‚   â””â”€â”€ README_ENVIRONMENTS.md      # DocumentaÈ›ie medii
â”‚
â”œâ”€â”€ experiments/                     # Scripturi experimentale
â”‚   â”œâ”€â”€ benchmark_all_agents.py     # Benchmark 5 algoritmi pe Easy
â”‚   â”œâ”€â”€ visualize_benchmark.py      # Generare 4 grafice benchmark
â”‚   â”œâ”€â”€ benchmark_multi_seed.py     # Experimente 5 seeds (reproducibilitate)
â”‚   â”œâ”€â”€ visualize_multi_seed.py     # Grafice multi-seed (3 plots)
â”‚   â”œâ”€â”€ test_easy_env.py            # Test rapid Q-Learning + DQN
â”‚   â””â”€â”€ test_dqn_per_dynamic.py     # Test DQN+PER pe Dynamic 8Ã—8
â”‚
â”œâ”€â”€ results/                         # Rezultate È™i grafice
â”‚   â”œâ”€â”€ benchmark_easy_*.json       # Date benchmark (JSON)
â”‚   â”œâ”€â”€ multi_seed_results.json     # Date multi-seed
â”‚   â”œâ”€â”€ benchmark_comparison.png    # ComparaÈ›ie 3 metrici
â”‚   â”œâ”€â”€ learning_curves.png         # Training progress
â”‚   â”œâ”€â”€ efficiency_scatter.png      # Scatter success vs steps
â”‚   â”œâ”€â”€ winner_ranking.png          # Ranking efficiency score
â”‚   â”œâ”€â”€ multi_seed_comparison.png   # Mean Â± std bars
â”‚   â”œâ”€â”€ multi_seed_stability.png    # Std comparison
â”‚   â””â”€â”€ multi_seed_distribution.png # Per-seed distribution
â”‚
â”œâ”€â”€ models/                          # AgenÈ›i antrenaÈ›i salvaÈ›i (opÈ›ional)
â”‚   â”œâ”€â”€ q_learning_*.pkl
â”‚   â”œâ”€â”€ dqn_per_*.pth
â”‚   â””â”€â”€ ppo_*.zip
â”‚
â”œâ”€â”€ .venv/                          # Virtual environment Python
â”‚
â”œâ”€â”€ requirements.txt                # DependenÈ›e Python
â”œâ”€â”€ README.md                       # Acest fiÈ™ier (documentaÈ›ie principalÄƒ)
â”œâ”€â”€ QUICKSTART.md                   # Ghid rapid de start
â”œâ”€â”€ INSTALL.md                      # Ghid detaliat instalare
â”œâ”€â”€ MULTI_SEED.md                   # Analiza reproducibilitÄƒÈ›ii
â”œâ”€â”€ IMPROVEMENTS.md                 # Extended documentation
â”‚
â”œâ”€â”€ test_setup.py                   # Script verificare instalare
â””â”€â”€ .gitignore                      # Git ignore rules
```

### Statistici Proiect

- **Linii de cod (Python):** ~3,500+ linii (fÄƒrÄƒ comentarii)
- **Linii documentaÈ›ie (Markdown):** ~2,000+ linii
- **NumÄƒr algoritmi:** 5 implementÄƒri complete
- **NumÄƒr medii:** 2 environments custom
- **Experimente rulate:** 2,500+ episoade evaluare (5 algoritmi Ã— 5 seeds Ã— 100 ep)
- **Grafice generate:** 7 tipuri diferite de vizualizÄƒri
- **Papers implementate:** 5 (Q-Learning, DQN, PER, PPO, RND)

---

## 8. ReferinÈ›e

### Papers Implementate

1. **Q-Learning**
   - Watkins, C. J., & Dayan, P. (1992). *Q-learning*. Machine learning, 8(3), 279-292.
   - [Link](https://link.springer.com/article/10.1007/BF00992698)

2. **DQN (Deep Q-Network)**
   - Mnih, V., et al. (2015). *Human-level control through deep reinforcement learning*. Nature, 518(7540), 529-533.
   - [Link](https://www.nature.com/articles/nature14236)

3. **Prioritized Experience Replay (PER)**
   - Schaul, T., et al. (2015). *Prioritized experience replay*. arXiv preprint arXiv:1511.05952.
   - [Link](https://arxiv.org/abs/1511.05952)

4. **PPO (Proximal Policy Optimization)**
   - Schulman, J., et al. (2017). *Proximal policy optimization algorithms*. arXiv preprint arXiv:1707.06347.
   - [Link](https://arxiv.org/abs/1707.06347)

5. **RND (Random Network Distillation)**
   - Burda, Y., et al. (2018). *Exploration by random network distillation*. arXiv preprint arXiv:1810.12894.
   - [Link](https://arxiv.org/abs/1810.12894)

### Papers Teoretice Utilizate

6. **Reward Shaping**
   - Ng, A. Y., et al. (1999). *Policy invariance under reward transformations: Theory and application to reward shaping*. ICML.
   - [Link](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf)

7. **GAE (Generalized Advantage Estimation)**
   - Schulman, J., et al. (2015). *High-dimensional continuous control using generalized advantage estimation*. arXiv:1506.02438.
   - [Link](https://arxiv.org/abs/1506.02438)

8. **Reproducibility in RL**
   - Henderson, P., et al. (2018). *Deep Reinforcement Learning that Matters*. AAAI.
   - [Link](https://arxiv.org/abs/1709.06560)

### Resurse È™i DocumentaÈ›ii

- **Gymnasium (OpenAI Gym successor):** [https://gymnasium.farama.org/](https://gymnasium.farama.org/)
- **Stable-Baselines3 (PPO implementation):** [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)
- **PyTorch RL Tutorials:** [https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)
- **Spinning Up in Deep RL (OpenAI):** [https://spinningup.openai.com/](https://spinningup.openai.com/)

### Bloguri È™i Tutoriale

- **Lilian Weng's RL Blog:** [https://lilianweng.github.io/posts/2018-02-19-rl-overview/](https://lilianweng.github.io/posts/2018-02-19-rl-overview/)
- **Andrej Karpathy - Pong from Pixels:** [http://karpathy.github.io/2016/05/31/rl/](http://karpathy.github.io/2016/05/31/rl/)
- **DeepMind Blog:** [https://deepmind.com/blog](https://deepmind.com/blog)

---

## LicenÈ›Äƒ

MIT License - vezi fiÈ™ierul LICENSE pentru detalii.

---

## Contact È™i ContribuÈ›ii

**Autor:** [Numele TÄƒu]
**Email:** [email@example.com]
**GitHub:** [https://github.com/username/proiect_irl](https://github.com/username/proiect_irl)

**ContribuÈ›ii:** Pull requests sunt binevenite! Pentru schimbÄƒri majore, deschide un issue mai Ã®ntÃ¢i.

---

## Acknowledgments

MulÈ›umiri pentru:
- **Stable-Baselines3** pentru implementarea PPO production-ready
- **Gymnasium** pentru framework-ul de environments
- **PyTorch** pentru deep learning infrastructure
- **OpenAI** pentru Spinning Up È™i resurse educaÈ›ionale
- **Comunitatea RL** pentru papers È™i open-source code

---

**Proiect realizat Ã®n cadrul cursului de Reinforcement Learning (2025)**
