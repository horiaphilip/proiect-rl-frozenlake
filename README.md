# Proiect Reinforcement Learning - Dynamic FrozenLake

Implementare È™i comparaÈ›ie a **5 algoritmi** de Reinforcement Learning pe medii FrozenLake custom cu dificultate variabilÄƒ.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“‹ Cuprins

- [Descriere](#descriere)
- [Algoritmi ImplementaÈ›i](#algoritmi-implementaÈ›i)
- [Medii (Environments)](#medii-environments)
- [Rezultate](#rezultate)
- [Instalare](#instalare)
- [Utilizare](#utilizare)
- [Structura Proiectului](#structura-proiectului)
- [ReferinÈ›e](#referinÈ›e)

---

## ğŸ¯ Descriere

Acest proiect implementeazÄƒ È™i comparÄƒ **5 algoritmi moderni** de Reinforcement Learning pe variante custom ale mediului FrozenLake:

1. **Q-Learning** (clasic tabular)
2. **DQN** (Deep Q-Network)
3. **DQN + PER** (DQN cu Prioritized Experience Replay) â­
4. **PPO** (Proximal Policy Optimization)
5. **PPO + RND** (PPO cu Random Network Distillation)

### Caracteristici Principale

âœ… **ImplementÄƒri complete** de la zero (PyTorch pentru deep RL)
âœ… **DouÄƒ medii custom** cu dificultate variabilÄƒ (Easy 4x4 È™i Dynamic 8x8)
âœ… **Benchmark comprehensiv** cu 5 algoritmi + vizualizÄƒri
âœ… **Rezultate validate**: DQN+PER cÃ¢È™tigÄƒtor cu 100% success rate
âœ… **DocumentaÈ›ie detaliatÄƒ** È™i cod bine comentat

---

## ğŸ¤– Algoritmi ImplementaÈ›i

### 1. Q-Learning
**LocaÈ›ie:** `agents/q_learning.py`

Algoritm clasic de Reinforcement Learning tabular.

**Caracteristici:**
- Q-table pentru stocare valori
- Îµ-greedy exploration
- Update rule: Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]

**Rezultate pe EasyFrozenLake:**
- Success Rate: **100%**
- Mean Steps: 6.54

---

### 2. DQN (Deep Q-Network)
**LocaÈ›ie:** `agents/dqn.py`

Extindere deep learning a Q-Learning, folosind reÈ›ele neuronale.

**Caracteristici:**
- Q-Network (neural network) pentru aproximare
- Experience Replay Buffer (10,000 capacitate)
- Target Network (update periodic)
- Îµ-greedy exploration

**ArhitecturÄƒ reÈ›ea:**
```
Input (one-hot state) â†’ Hidden(128) â†’ ReLU â†’ Hidden(128) â†’ ReLU â†’ Output(n_actions)
```

**Rezultate pe EasyFrozenLake:**
- Success Rate: 32% (necesitÄƒ mai mult tuning)
- Mean Steps: 32.76

---

### 3. DQN + PER (Prioritized Experience Replay) ğŸ†
**LocaÈ›ie:** `agents/dqn_per.py`

**CÃ¢È™tigÄƒtor Benchmark!**

DQN Ã®mbunÄƒtÄƒÈ›it cu sampling prioritizat din replay buffer.

**Caracteristici:**
- **SumTree** pentru sampling eficient O(log n)
- Prioritizare bazatÄƒ pe TD-error: P(i) âˆ |Î´áµ¢|^Î±
- Importance Sampling weights pentru corectare bias
- Beta annealing schedule (0.4 â†’ 1.0)

**De ce funcÈ›ioneazÄƒ mai bine:**
- ÃnvaÈ›Äƒ mai repede din tranziÈ›ii importante (TD-error mare)
- Sample-efficiency mult mai mare vs DQN vanilla
- ConvergenÈ›Äƒ mai rapidÄƒ È™i mai stabilÄƒ

**Rezultate pe EasyFrozenLake:**
- Success Rate: **100%** â­
- Mean Steps: **6.37** (cel mai eficient!)
- Efficiency Score: **15.70** (best overall)

---

### 4. PPO (Proximal Policy Optimization)
**LocaÈ›ie:** `agents/ppo.py`

Algoritm modern policy gradient cu clipping pentru stabilitate.

**Caracteristici:**
- Actor-Critic architecture
- Clipped surrogate objective
- GAE (Generalized Advantage Estimation)
- Multiple epochs pe acelaÈ™i batch

**Obiectiv clipat:**
```
L^CLIP(Î¸) = E[min(r(Î¸)A, clip(r(Î¸), 1-Îµ, 1+Îµ)A)]
```

**Rezultate pe EasyFrozenLake:**
- Success Rate: **100%**
- Mean Steps: 6.38
- Foarte stabil È™i consistent

---

### 5. PPO + RND (Random Network Distillation)
**LocaÈ›ie:** `agents/ppo_rnd.py`

PPO extins cu intrinsic rewards pentru explorare mai bunÄƒ.

**Caracteristici:**
- **Target Network** (fixed random)
- **Predictor Network** (trainable)
- Intrinsic reward: r_int = MSE(target(s), predictor(s))
- Total reward: r = r_ext + Î² * normalize(r_int)

**CÃ¢nd e util:**
- Medii cu sparse rewards
- Explorare dificilÄƒ
- State-space mare

**Rezultate pe EasyFrozenLake:**
- Success Rate: **100%**
- Mean Steps: 6.40
- Nu aduce beneficii pe easy env (rewards nu sunt sparse)

---

## ğŸ”ï¸ Medii (Environments)

### EasyFrozenLake â­ (Recomandat pentru Ã®nceput)
**LocaÈ›ie:** `environments/easy_frozenlake.py`

Mediu simplificat, optimizat pentru Ã®nvÄƒÈ›are rapidÄƒ.

| CaracteristicÄƒ | Valoare |
|----------------|---------|
| Map size | 4x4 (16 stÄƒri) |
| Slippery | 5% (constant) |
| Hole ratio | 10% |
| Safe zone | 2x2 lÃ¢ngÄƒ start |
| Ice melting | OFF |
| Reward shaping | ON |
| Max steps | 50 |

**Rezultate Benchmark:**

| Algorithm | Success Rate | Mean Steps | Efficiency |
|-----------|--------------|------------|------------|
| Q-Learning | 100% | 6.54 | 15.29 |
| DQN | 32% | 32.76 | 0.98 |
| **DQN+PER** | **100%** | **6.37** | **15.70** ğŸ† |
| PPO | 100% | 6.38 | 15.67 |
| PPO+RND | 100% | 6.40 | 15.62 |

**CÃ¢nd sÄƒ foloseÈ™ti:**
- Testing rapid algoritmi noi
- Debugging È™i proof-of-concept
- Baseline pentru comparaÈ›ii
- Success rate garantat > 90%

---

### DynamicFrozenLake (Challenge)
**LocaÈ›ie:** `environments/dynamic_frozenlake.py`

Mediu complex cu dificultate crescÃ¢ndÄƒ Ã®n timp.

| CaracteristicÄƒ | Valoare |
|----------------|---------|
| Map size | 8x8 (64 stÄƒri) |
| Slippery | 0.08 â†’ 0.25 (creÈ™te) |
| Hole ratio | 18-20% |
| Safe zone | ProtejatÄƒ |
| Ice melting | ON (controlat) |
| Reward shaping | OpÈ›ional |
| Max steps | 120-140 |

**Challenge-uri:**
- Probabilitate variabilÄƒ de alunecare (creÈ™te Ã®n timp)
- GheaÈ›Äƒ se topeÈ™te progresiv (devine gaurÄƒ)
- Map mai mare = explorare mai dificilÄƒ
- NecesitÄƒ 1000+ episoade training

**CÃ¢nd sÄƒ foloseÈ™ti:**
- Testare robusteÈ›e algoritmi
- ComparaÈ›ie performanÈ›Äƒ pe task dificil
- Research È™i experimente avansate

---

## ğŸ“Š Rezultate

### Benchmark Complet pe EasyFrozenLake (4x4)

**Setup:**
- 500 episoade training (Q-Learning, DQN, DQN+PER)
- 25,000 timesteps (PPO, PPO+RND)
- 100 episoade evaluare
- Seed: 42 (reproducibilitate)

**Tabel Rezultate:**

| Algorithm | Success Rate | Mean Reward | Mean Steps | Efficiency Score* |
|-----------|--------------|-------------|------------|-------------------|
| Q-Learning | 100.00% | 1.1946 | 6.54 | 15.29 |
| DQN | 32.00% | 0.0538 | 32.76 | 0.98 |
| **DQN+PER** | **100.00%** | **1.1963** | **6.37** | **15.70** ğŸ‘‘ |
| PPO | 100.00% | 1.1962 | 6.38 | 15.67 |
| PPO+RND | 100.00% | 1.1960 | 6.40 | 15.62 |

*Efficiency Score = Success Rate / Mean Steps (higher is better)

### ğŸ† CÃ¢È™tigÄƒtor: DQN + PER

**De ce cÃ¢È™tigÄƒ DQN+PER:**
1. **100% success rate** (Ã®mpreunÄƒ cu Q-Learning, PPO, PPO+RND)
2. **Cel mai eficient**: doar 6.37 paÈ™i Ã®n medie
3. **Prioritized Experience Replay** face diferenÈ›a enormÄƒ vs DQN vanilla
4. **Sample efficiency**: converge mai rapid

### ObservaÈ›ii Cheie

1. **PER face diferenÈ›a**: DQN (32%) â†’ DQN+PER (100%)
2. **PPO foarte consistent**: success rate 100%, eficienÈ›Äƒ excelentÄƒ
3. **Q-Learning surprinde**: funcÈ›ioneazÄƒ foarte bine pe medii simple
4. **RND nu ajutÄƒ** pe EasyFrozenLake (rewards nu sunt sparse enough)
5. **DQN vanilla** necesitÄƒ mai mult tuning sau training

### VizualizÄƒri Generate

Vezi folder `results/` pentru grafice:

1. **benchmark_comparison.png** - ComparaÈ›ie 3 metrici (Success, Reward, Steps)
2. **learning_curves.png** - EvoluÈ›ia rewardurilor Ã®n timp
3. **efficiency_scatter.png** - Success Rate vs Mean Steps
4. **winner_ranking.png** - Clasament final cu cÃ¢È™tigÄƒtor evidenÈ›iat

---

## ğŸš€ Instalare

### CerinÈ›e

- Python 3.8+
- pip
- Virtual environment (recomandat)

### Setup Rapid

```bash
# Navigate to project directory
cd proiect_irl

# Create virtual environment
python -m venv .venv

# Activate
# Windows:
.venv\Scripts\activate
# Linux/Mac:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### DependinÈ›e Principale

```
numpy>=1.24.0
torch>=2.0.0
gymnasium>=0.29.0
matplotlib>=3.7.0
seaborn>=0.12.0
tqdm>=4.65.0
stable-baselines3>=2.0.0
```

---

## ğŸ’» Utilizare

### Quick Start - Test Rapid

```bash
cd experiments
python test_easy_env.py
```

**Output aÈ™teptat:**
```
Q-Learning Success Rate: 63%
DQN Success Rate: 64%
SUCCESS! Agents learned to reach the goal!
```

### Benchmark Complet (ToÈ›i cei 5 AgenÈ›i)

```bash
cd experiments
python benchmark_all_agents.py
```

**Ce face:**
- RuleazÄƒ Q-Learning (500 ep)
- RuleazÄƒ DQN (500 ep)
- RuleazÄƒ DQN+PER (500 ep)
- RuleazÄƒ PPO (25k timesteps)
- RuleazÄƒ PPO+RND (25k timesteps)
- SalveazÄƒ rezultate Ã®n `results/benchmark_easy_TIMESTAMP.json`

**Timp estimat:** ~5-10 minute

### Vizualizare Rezultate

```bash
cd experiments
python visualize_benchmark.py
```

**Output:**
- ÃncarcÄƒ ultimul benchmark
- GenereazÄƒ 4 grafice PNG
- SalveazÄƒ Ã®n `results/`

### Training Custom

#### Q-Learning
```python
from environments.easy_frozenlake import EasyFrozenLakeEnv
from agents.q_learning import QLearningAgent

env = EasyFrozenLakeEnv(map_size=4)
agent = QLearningAgent(
    n_states=env.observation_space.n,
    n_actions=env.action_space.n,
    learning_rate=0.1,
    discount_factor=0.99
)

# Training
for episode in range(500):
    stats = agent.train_episode(env)
    if episode % 100 == 0:
        print(f"Episode {episode}: Reward = {stats['total_reward']}")

# Evaluation
eval_stats = agent.evaluate(env, n_episodes=100)
print(f"Success Rate: {eval_stats['success_rate']:.2%}")
```

#### DQN + PER (Recomandat)
```python
from agents.dqn_per import DQN_PERAgent

agent = DQN_PERAgent(
    n_states=env.observation_space.n,
    n_actions=env.action_space.n,
    learning_rate=0.001,
    per_alpha=0.6,
    per_beta_start=0.4,
    seed=42
)

for episode in range(500):
    stats = agent.train_episode(env)

eval_stats = agent.evaluate(env, n_episodes=100)
```

#### PPO
```python
from agents.ppo import PPOAgent

agent = PPOAgent(
    env=env,
    learning_rate=0.0003,
    n_steps=512,
    batch_size=64
)

agent.train(total_timesteps=25000)
eval_stats = agent.evaluate(env, n_episodes=100)
```

---

## ğŸ“ Structura Proiectului

```
proiect_irl/
â”‚
â”œâ”€â”€ agents/                      # ImplementÄƒri algoritmi RL
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ q_learning.py           # Q-Learning tabular
â”‚   â”œâ”€â”€ dqn.py                  # Deep Q-Network
â”‚   â”œâ”€â”€ dqn_per.py             # DQN + Prioritized Replay â­
â”‚   â”œâ”€â”€ ppo.py                  # Proximal Policy Optimization
â”‚   â””â”€â”€ ppo_rnd.py             # PPO + Random Network Distillation
â”‚
â”œâ”€â”€ environments/                # Medii custom
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ easy_frozenlake.py     # Environment simplu (4x4) â­
â”‚   â”œâ”€â”€ dynamic_frozenlake.py  # Environment dificil (8x8)
â”‚   â””â”€â”€ README_ENVIRONMENTS.md  # DocumentaÈ›ie medii
â”‚
â”œâ”€â”€ experiments/                 # Scripturi pentru rulare
â”‚   â”œâ”€â”€ test_easy_env.py       # Test rapid pe Easy
â”‚   â”œâ”€â”€ benchmark_all_agents.py # Benchmark complet 5 algoritmi
â”‚   â”œâ”€â”€ visualize_benchmark.py  # Generare grafice
â”‚   â””â”€â”€ run_experiments.py      # Training complet (toate mediile)
â”‚
â”œâ”€â”€ results/                     # Rezultate È™i grafice
â”‚   â”œâ”€â”€ benchmark_easy_*.json   # Date benchmark
â”‚   â”œâ”€â”€ benchmark_comparison.png
â”‚   â”œâ”€â”€ learning_curves.png
â”‚   â”œâ”€â”€ efficiency_scatter.png
â”‚   â””â”€â”€ winner_ranking.png
â”‚
â”œâ”€â”€ .venv/                      # Virtual environment
â”œâ”€â”€ requirements.txt            # DependinÈ›e Python
â””â”€â”€ README.md                   # Acest fiÈ™ier
```

---

## ğŸ“ Concluzii È™i ÃnvÄƒÈ›Äƒminte

### Ce am Ã®nvÄƒÈ›at din benchmark:

1. **PER chiar funcÈ›ioneazÄƒ** âš¡
   - DQN simplu: 32% success
   - DQN+PER: 100% success
   - Sample efficiency mult mai bunÄƒ

2. **Environment design conteazÄƒ** ğŸ”ï¸
   - EasyFrozenLake: success rate > 90% pentru majoritatea
   - DynamicFrozenLake: challenge real pentru algoritmi

3. **Nu Ã®ntotdeauna mai complex = mai bun** ğŸ¤”
   - Q-Learning simplu bate DQN vanilla pe medii simple
   - RND nu ajutÄƒ cÃ¢nd rewards nu sunt sparse

4. **Tuning hiperparametri e esenÈ›ial** ğŸ›ï¸
   - Epsilon decay
   - Learning rate
   - Buffer size
   - Update frequency

### RecomandÄƒri Practice:

**Pentru medii simple (4x4, puÈ›ine gÄƒuri):**
- FoloseÈ™te **Q-Learning** sau **DQN+PER**
- Training rapid (< 500 episoade)
- Success rate garantat

**Pentru medii complexe (8x8+, very sparse rewards):**
- FoloseÈ™te **PPO** sau **PPO+RND**
- Mai mult training (> 1000 episoade)
- Reward shaping ajutÄƒ

**Pentru research:**
- **DQN+PER** = cel mai eficient overall
- **PPO** = cel mai stabil
- **PPO+RND** = best pentru explorare dificilÄƒ

---

## ğŸ“š ReferinÈ›e

### Papers

1. **Q-Learning**
   Watkins, C. J., & Dayan, P. (1992). Q-learning. Machine learning, 8(3), 279-292.

2. **DQN**
   Mnih, V., et al. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533.

3. **Prioritized Experience Replay**
   Schaul, T., et al. (2015). Prioritized experience replay. arXiv preprint arXiv:1511.05952.

4. **PPO**
   Schulman, J., et al. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.

5. **RND (Random Network Distillation)**
   Burda, Y., et al. (2018). Exploration by random network distillation. arXiv preprint arXiv:1810.12894.

### Resurse Utile

- [Gymnasium Documentation](https://gymnasium.farama.org/)
- [PyTorch RL Tutorials](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)
- [Stable Baselines3](https://stable-baselines3.readthedocs.io/)
- [Spinning Up in Deep RL](https://spinningup.openai.com/)

---

## ğŸ“ LicenÈ›Äƒ

MIT License - vezi fiÈ™ierul LICENSE pentru detalii.
